[
	{
		"title": "Scalable Ray Tracing Using the Distributed FrameBuffer",
		"authors": "Will Usher, Ingo Wald, Jefferson Amstutz, Johannes Günther,
			Carson Brownlee, and Valerio Pascucci",
		"venue": "Computer Graphics Forum (Proceedings of EuroVis)",
		"paper_pdf": "http://sci.utah.edu/~will/papers/dfb.pdf",
		"teaser": "https://i.imgur.com/xDw1wI1.jpg",
		"thumb": "https://i.imgur.com/xVd40uT.jpg",
		"year": 2019,
		"short_title": "dfb",
		"supplemental_video": "https://www.youtube.com/embed/F2nahk4GAB0",
		"doi": "10.1111/cgf.13702",
		"bibtex": "@article{usher_scalable_2019,\n
			journal = {Computer Graphics Forum},\n
			title = {{Scalable Ray Tracing Using the Distributed FrameBuffer}},\n
			author = {Usher, Will and Wald, Ingo and Amstutz, Jefferson and
				Günther, Johannes and Brownlee, Carson and Pascucci, Valerio},\n
			year = {2019},\n
			publisher = {The Eurographics Association and John Wiley & Sons Ltd.},\n
			ISSN = {1467-8659},\n
			DOI = {10.1111/cgf.13702}\n}",
		"abstract": "Image- and data-parallel rendering across multiple
			nodes on high-performance computing systems is widely used in visualization
			to provide higher frame rates, support large data sets, and
			render data in situ. Specifically for in situ visualization,
			reducing bottlenecks incurred by the visualization and compositing is
			of key concern to reduce the overall simulation runtime.
			Moreover, prior algorithms have been designed to support either
			image- or data-parallel rendering and impose restrictions on the
			data distribution, requiring different implementations for each
			configuration. In this paper, we introduce the Distributed FrameBuffer,
			an asynchronous image-processing framework for multi-node rendering.
			We demonstrate that our approach achieves performance superior to
			the state of the art for common use cases, while providing the
			flexibility to support a wide range of parallel rendering algorithms
			and data distributions. By building on this framework, we extend
			the open-source ray tracing library OSPRay with a data-distributed API, enabling
			its use in data-distributed and in situ visualization applications.",
		"teaser_caption": "Large-scale interactive visualization using the Distributed FrameBuffer.
			Top left: Image-parallel rendering of two transparent isosurfaces from
			the Richtmyer-Meshkov (516M triangles), 8FPS with a 2048<sup>2</sup>
			framebuffer using 16 Stampede2 Intel Xeon Platinum 8160 SKX nodes.
			Top right: Data-parallel rendering of the Cosmic Web (29B transparent spheres),
			2FPS at 2048<sup>2</sup> using 128 Theta Intel Xeon Phi Knight's Landing (KNL) nodes.
			Bottom: Data-parallel rendering of the 951GB DNS volume combined with a transparent
			isosurface (4.35B triangles), 5FPS at 4096x1024 using 64 Stampede2 Intel Xeon Phi
			KNL nodes.",
		"downloads": [
			{
				"title": "Code",
				"list": [
					{
						"title": "OSPRay",
						"link": "https://github.com/ospray/ospray",
						"icon": "fab fa-github"
					},
					{
						"title": "Distributed Rendering Benchmark App",
						"link": "https://github.com/Twinklebear/osp-distrib-viewer",
						"icon": "fab fa-github"
					},
					{
						"title": "IceT Comparison App",
						"link": "https://github.com/Twinklebear/osp-icet",
						"icon": "fab fa-github"
					}
				]
			}
		]
	},
	{
		"title": "Ray Tracing Generalized Tube Primitives: Method and Applications",
		"authors": "Mengjiao Han, Ingo Wald, Will Usher, Qi Wu, Feng Wang,
			Valerio Pascucci, Charles D. Hansen, and Chris R. Johnson",
		"venue": "Computer Graphics Forum (Proceedings of EuroVis)",
		"paper_pdf": "http://sci.utah.edu/~will/papers/tubes.pdf",
		"teaser": "https://i.imgur.com/tTv2l2j.jpg",
		"thumb": "https://i.imgur.com/gsgdwlZ.jpg",
		"year": 2019,
		"short_title": "tubes",
		"doi": "10.1111/cgf.13703",
		"bibtex": "@article{han_ray_2019,\n
			journal = {Computer Graphics Forum},\n
			title = {{Ray Tracing Generalized Tube Primitives: Method and Applications}},\n
			author = {Han, Mengjiao and Wald, Ingo and Usher, Will and
				Wu, Qi and Wang, Feng and Pascucci, Valerio and Hansen, Charles D.
				and Johnson, Chris R.},\n
			year = {2019},\n
			publisher = {The Eurographics Association and John Wiley & Sons Ltd.},\n
			ISSN = {1467-8659},\n
			DOI = {10.1111/cgf.13703}\n}",
		"abstract": "We present a general high-performance technique for ray tracing
			generalized tube primitives. Our technique efficiently supports
			tube primitives with fixed and varying radii, general acyclic graph
			structures with bifurcations,
			and correct transparency with interior surface removal.
			Such tube primitives are widely used in scientific visualization 
			to represent diffusion tensor imaging tractographies, neuron morphologies,
			and scalar or vector fields of 3D flow.
			We implement our approach within the OSPRay ray tracing framework,
			and evaluate it on a range of interactive visualization use cases
			of fixed- and varying-radius streamlines, pathlines, complex neuron morphologies,
			and brain tractographies. Our proposed approach provides interactive,
			high-quality rendering, with low memory overhead.",
		"teaser_caption": "Visualizations using our \"generalized tube\" primitives.
			(a): DTI tractography data, semi-transparent fixed-radius streamlines (218K line segments). 
			(b): A generated neuron assembly test case, streamlines with varying
			radii and bifurcations (3.2M l. s.).
			(c): Aneurysm morphology, semi-transparent streamlines with varying radii
			and bifurcations (3.9K l. s.)
			and an opaque center line with fixed radius and bifurcations (3.9K l. s.).
			(d): A tornado simulation, with radius used to encode the velocity magnitude (3.56M l. s.).
			(e): Flow past a torus, fixed-radius pathlines (6.5M l. s.).
			Rendered at: (a) 0.38FPS, (b) 7.2FPS, (c) 0.25FPS, (d) 18.8FPS, with a 2048x2048 framebuffer;
			(e) 23FPS with a 2048x786 framebuffer. Performance measured on a dual Intel Xeon
			E5-2640 v4 workstation, with shadows and ambient occlusion.",
		"supplemental_video": "https://www.youtube.com/embed/RB2yC5Io3JA",
		"downloads": [
			{
				"title": "Code",
				"list": [
					{
						"title": "OSPRay Module",
						"link": "https://github.com/MengjiaoH/ospray-module-tubes",
						"icon": "fab fa-github"
					}
				]
			}
		]
	},
	{
		"title": "Efficient Space Skipping and Adaptive Sampling of Unstructured
			Volumes Using Hardware Accelerated Ray Tracing",
		"authors": "Nate Morrical, Will Usher, Ingo Wald and Valerio Pascucci",
		"venue": "To Appear, IEEE VIS Short Papers",
		"paper_pdf": "http://sci.utah.edu/~will/papers/vis19-space-skipping-short.pdf",
		"teaser": "https://i.imgur.com/NgoH3iw.jpg",
		"thumb": "https://i.imgur.com/deoKQwV.jpg",
		"year": 2019,
		"short_title": "rtx-space-skipping",
		"bibtex": "@inproceedings{morrical_skipping_2019,\n
			title = {Efficient {Space} {Skipping} and {Adaptive} {Sampling} of {Unstructured}
				{Volumes} {Using} {Hardware} {Accelerated} {Ray} {Tracing}},\n
			booktitle = {IEEE VIS 2019 - Short Papers},\n
			author = {Morrical, Nate, and Usher, Will and Wald, Ingo, and Pascucci, Valerio},\n
			year = {2019},\n}",
		"abstract": "Sample based ray marching is an effective method for direct volume
			rendering of unstructured meshes. However, sampling such meshes remains expensive,
			and strategies to reduce the number of samples taken have received relatively
			little attention. In this paper, we introduce a method for rendering unstructured 
			meshes using a combination of a coarse spatial acceleration structure and 
			hardware-accelerated ray tracing. Our approach enables efficient empty space
			skipping and adaptive sampling of unstructured meshes, and outperforms a reference
			ray marcher by up to 7×",
		"teaser_caption": "Performance improvement of our method on the 278 million
			tetrahedra Japan Earthquake data set. (a) A reference volume ray marcher
			without our method, at 0.9 FPS (1024<sup>2</sup> pixels) on an NVIDIA RTX 8000 GPU.
			(b) A heat map of relative cost per-pixel in (a).
			(c) and (d), the same, but now with our space skipping and adaptive sampling method,
			running at 7 FPS (7× faster)."
	},
	{
		"title": "RTX Beyond Ray Tracing: Exploring the Use of Hardware Ray Tracing
			Cores for Tet-Mesh Point Location",
		"authors": "Ingo Wald, Will Usher, Nate Morrical, Laura Lediaev, and Valerio Pascucci",
		"venue": "High Performance Graphics Short Papers",
		"paper_pdf": "http://sci.utah.edu/~will/papers/rtx-points-hpg19.pdf",
		"teaser": "https://i.imgur.com/clMvtsl.png",
		"thumb": "https://i.imgur.com/nbHnD45.jpg",
		"year": 2019,
		"short_title": "rtx-points",
		"doi": "10.2312/hpg.20191189",
		"bibtex": "@inproceedings{wald_rtx_2019,\n
			title = {{RTX} {Beyond} {Ray} {Tracing:} {Exploring} the {Use} of {Hardware}
				{Ray} {Tracing} {Cores} for {Tet}-{Mesh} {Point} {Location}},\n
			booktitle = {High-Performance Graphics - Short Papers},\n
			author = {Wald, Ingo and Usher, Will and Morrical, Nate
				and Lediaev, Laura and Pascucci, Valerio},\n
			year = {2019},\n
			DOI = {10.2312/hpg.20191189}\n}",
		"abstract": "We explore a first proof-of-concept example of creatively using the
			Turing generation's hardware ray tracing cores to solve a problem
			other than classical ray tracing, specifically, point location in
			unstructured tetrahedral meshes. Starting with a CUDA reference
			method, we describe and evaluate three different approaches to
			reformulate this problem in a manner that allows it to be mapped
			to these new hardware units. Each variant replaces the simpler problem
			of point queries with the more complex one of ray queries; however,
			thanks to hardware acceleration, these approaches are actually
			faster than the reference method.",
		"teaser_caption": "a-c) Illustrations of the tetrahedral mesh point location
		kernels evaluated in this paper. a) Our reference method builds a BVH over
		the tets and performs both BVH traversal and point-in-tet tests in software
		(black) using CUDA. b) <font face='monospace'>rtx-bvh</font> uses an
		RTX-accelerated BVH over tets and
		triggers hardware BVH traversal (green) by tracing infinitesimal rays at the
		sample points, while still performing point-tet tests in software (black).
		c) <font face='monospace'>rtx-rep-faces</font> and
		<font face='monospace'>rtx-shrd-faces</font> use both hardware BVH traversal and
		triangle intersection (green) by tracing rays against the tetrahedras' faces.
		d) An image from the unstructured-data volume ray marcher used to evaluate
		our point location kernels, showing the 35.7M tet Agulhas Current data
		set rendered interactively on an NVIDIA TITAN RTX (34FPS at 1024<sup>2</sup> pixels)"
	},
	{
		"title": "Spatially-aware Parallel I/O for Particle Data",
		"authors": "Sidharth Kumar, Steve Petruzza, Will Usher, and Valerio Pascucci",
		"venue": "International Conference on Parallel Processing (ICPP) (To Appear)",
		"paper_pdf": "http://sci.utah.edu/~will/papers/icpp19.pdf",
		"teaser": "/assets/img/icpp-two-phase-io.svg",
		"thumb": "https://i.imgur.com/GWCom0N.jpg",
		"year": 2019,
		"short_title": "icpp19",
		"bibtex": "@inproceedings{kumar_spatially-aware_2019,\n
			title = {{Spatially}-aware {Parallel} {I}/{O} for {Particle} {Data}},\n
			booktitle = {48th International Conference on Parallel Processing (ICPP 2019) (To Appear)},\n
			author = {Kumar, Sidharth and Petruzza, Steve and Usher, Will and Pascucci, Valerio},\n
			year = {2019},\n}",
		"abstract": "Particle data are used across a diverse set of large scale simulations,
			for example, in cosmology, molecular dynamics and combustion. 
			At scale these applications generate tremendous amounts of data, which is
			often saved in an unstructured format that does not preserve spatial locality;
			resulting in poor read performance for post-processing analysis and visualization tasks,
			which typically make spatial queries.
			In this work, we explore some of the challenges of large scale particle data 
			management, and introduce new techniques to perform scalable, spatially-aware write and read operations.
			We propose an adaptive aggregation technique to improve the performance of data aggregation, for
			both uniform and non-uniform particle distributions. Furthermore, we enable efficient read operations
			by employing a level of detail re-ordering and a multi-resolution layout. Finally, we demonstrate the
			scalability of our techniques with experiments
			on large scale simulation workloads up to 256K cores on two different leadership supercomputers, Mira and Theta.",
		"teaser_caption": "An illustration of our two-phase I/O approach, which takes spatial locality into consideration."
	},
	{
		"title": "CPU Isosurface Ray Tracing of Adaptive Mesh Refinement Data",
		"authors": "Feng Wang, Ingo Wald, Qi Wu, Will Usher and Chris R. Johnson",
		"venue": "IEEE Transactions on Visualization and Computer Graphics",
		"paper_pdf": "http://sci.utah.edu/~will/papers/amr-isosurface.pdf",
		"teaser": "https://i.imgur.com/PqmRTuz.jpg",
		"thumb": "https://i.imgur.com/Yzxf5OT.jpg",
		"year": 2019,
		"short_title": "amr-iso",
		"doi": "10.1109/TVCG.2018.2864850",
		"bibtex": "@article{Wang_AMR_Iso_2019,\n
			author={F. Wang and I. Wald and Q. Wu and W. Usher and C. R. Johnson},\n
			journal={{IEEE} {Transactions} on {Visualization} and {Computer} {Graphics}},\n
			title={{CPU} {Isosurface} {Ray} {Tracing} of {Adaptive} {Mesh} {Refinement} {Data}},\n
			year={2019},\n
			doi={10.1109/TVCG.2018.2864850},\n}",
		"abstract": "Adaptive mesh refinement (AMR) is a key technology for large-scale simulations that allows for adaptively changing the simulation mesh resolution, resulting in significant computational and storage savings. However, visualizing such AMR data poses a significant challenge due to the difficulties introduced by the hierarchical representation when reconstructing continuous field values. In this paper, we detail a comprehensive solution for interactive isosurface rendering of block-structured AMR data. We contribute a novel reconstruction strategy&mdash;the <i>octant</i> method&mdash;which is continuous, adaptive and simple to implement. Furthermore, we present a generally applicable hybrid implicit isosurface ray-tracing method, which provides better rendering quality and performance than the built-in sampling-based approach in OSPRay. Finally, we integrate our <i>octant</i> method and hybrid isosurface geometry into OSPRay as a module, providing the ability to create high-quality interactive visualizations combining volume and isosurface representations of BS-AMR data. We evaluate the rendering performance, memory consumption and quality of our method on two gigascale block-structured AMR datasets.",
		"teaser_caption": "High-fidelity isosurface visualizations of gigascale block-structured adaptive mesh refinement (BS-AMR) data using our method. Left: a 28GB GR-Chombo simulation of gravitational waves resulting from the collision of two black holes. Middle and Right: a 57GB AMR dataset computed with LAVA at NASA, simulating multiple fields over the landing gear of an aircraft. Middle: isosurface representation of the vorticity, rendered with path tracing. Right: a combined visualization of volume rending and an isosurface of the pressure over the landing gear, rendered with OSPRay's SciVis renderer.  Using our approach for ray tracing such AMR data, we can interactively render crack-free implicit isosurfaces in combination with direct volume rendering and advanced shading effects like transparency, ambient occlusion and path tracing.",
		"downloads": [
			{
				"title": "Code",
				"list": [
					{
						"title": "AMR Isosurface module for OSPRay",
						"link": "https://github.com/ethan0911/module-impi",
						"icon": "fab fa-github"
					}
				]
			}
		]
	},
	{
		"title": "A Virtual Reality Visualization Tool for Neuron Tracing",
		"authors": "Will Usher, Pavol Klacansky, Frederick Federer, Peer-Timo Bremer,
				Aaron Knoll, Jeff Yarch, Alessandra Angelucci, and Valerio Pascucci",
		"venue": "IEEE Transactions on Visualization and Computer Graphics",
		"paper_pdf": "http://sci.utah.edu/~will/papers/vrnt/vr-neuron-tracing.pdf",
		"teaser": "https://i.imgur.com/2rmTeh2.png",
		"thumb": "https://i.imgur.com/2iL7UdK.png",
		"year": 2018,
		"short_title": "vrnt",
		"doi": "10.1109/TVCG.2017.2744079",
		"supplemental_video": "https://www.youtube.com/embed/lTNef_kbLKg",
		"presentation_video": "https://www.youtube.com/embed/XzmYdl9rFs0",
		"downloads": [
			{
				"title": "Videos",
				"list": [
					{
						"title": "Analyzing an Expert Session",
						"link": "https://youtu.be/L5tsU8TtgkU"
					},
					{
						"title": "ZED Mixed Reality Tracing Video",
						"link": "https://youtu.be/qIPW3ut4sv8"
					},
					{
						"title": "ZED Mixed Reality Prototype",
						"link": "https://youtu.be/t6rPU6hy5tk"
					},
					{
						"title": "VIS17 Fast Forward",
						"link": "https://youtu.be/mZ6YT_y7Kx0"
					}
				]
			},
			{
				"title": "Software",
				"list": [
					{
						"title": "Available on Steam!",
						"link": "https://store.steampowered.com/app/791040/Virtual_Reality_Neuron_Tracer/",
						"icon": "fab fa-steam"
					}
				]
			}
		],
		"bibtex": "@article{Usher_VRNT_2018,\n
			author={W. Usher and P. Klacansky and F. Federer and P. T. Bremer and A. Knoll and J. Yarch and A. Angelucci and V. Pascucci},\n
			journal={{IEEE} {Transactions} on {Visualization} and {Computer} {Graphics}},\n
			title={A {Virtual} {Reality} {Visualization} {Tool} for {Neuron} {Tracing}},\n
			year={2018},\n
			volume={24},\n
			number={1},\n
			pages={994-1003},\n
			doi={10.1109/TVCG.2017.2744079},\n
			ISSN={1077-2626},\n
			month={Jan},\n}",
		"abstract": "Tracing neurons in large-scale microscopy data is crucial to establishing a wiring diagram of the brain, which is needed to understand how neural circuits in the brain process information and generate behavior. Automatic techniques often fail for large and complex datasets, and connectomics researchers may spend weeks or months manually tracing neurons using 2D image stacks. We present a design study of a new virtual reality (VR) system, developed in collaboration with trained neuroanatomists, to trace neurons in microscope scans of the visual cortex of primates. We hypothesize that using consumer-grade VR technology to interact with neurons directly in 3D will help neuroscientists better resolve complex cases and enable them to trace neurons faster and with less physical and mental strain. We discuss both the design process and technical challenges in developing an interactive system to navigate and manipulate terabyte-sized image volumes in VR. Using a number of different datasets, we demonstrate that, compared to widely used commercial software, consumer-grade VR presents a promising alternative for scientists.",
		"teaser_caption": "A screenshot of our VR neuron tracing tool using the isosurface rendering mode. The dark gray floor represents the extent of the tracked space. Users can orient themselves in the dataset via the minimap (right), which shows the world extent in blue, the current focus region in orange, and the previously traced neuronal structures. The focus region is displayed in the center of the space. The 3D interaction and visualization provides an intuitive environment for exploring the data and a natural interface for neuron tracing, resulting in faster, high-quality traces with less fatigue reported by users compared to existing 2D tools."
	},
	{
		"title": "libIS: A Lightweight Library for Flexible In Transit Visualization",
		"authors": "Will Usher, Silvio Rizzi, Ingo Wald, Jefferson Amstutz,
			Joseph Insley, Venkatram Vishwanath, Nicola Ferrier,
			Michael E. Papka, and Valerio Pascucci",
		"venue": "ISAV: In Situ Infrastructures for Enabling Extreme-Scale
			Analysis and Visualization (ISAV '18)",
		"paper_pdf": "http://sci.utah.edu/~will/papers/libis-isav18.pdf",
		"teaser": "https://i.imgur.com/UYlTqhT.png",
		"thumb": "https://i.imgur.com/aCX8XH2.png",
		"year": 2018,
		"short_title": "libis-isav18",
		"doi": "10.1145/3281464.3281466",
		"supplemental_video": "https://www.youtube.com/embed/YUH55CvPmxg",
		"bibtex": "@inproceedings{usher_libis_2018,\n
			title = {{libIS}: {A} {Lightweight} {Library} for
				{Flexible} {In} {Transit} {Visualization}},\n
			year = {2018},\n
			booktitle = {ISAV: In Situ Infrastructures for Enabling
				Extreme-Scale Analysis and Visualization},\n
			series = {ISAV'18},\n
			author = {Usher, Will and Rizzi, Silvio and Wald, Ingo
				and Amstutz, Jefferson and Insley, Joseph and
				Vishwanath, Venkatram and Ferrier, Nicola and
				Papka, Michael E. and Pascucci, Valerio},\n
			doi={10.1145/3281464.3281466}\n
		}",
		"teaser_caption": "Interactive in situ visualization of a 172k atom simulation of silicene formation with 128 LAMMPS ranks sending to 16 OSPRay renderer ranks, all executed on Theta in the mpi-multi configuration. When taking four ambient occlusion samples per-pixel, our viewer averages 7FPS at 1024x1024. Simulation dataset is courtesy of <a href='https://pubs.rsc.org/en/content/articlelanding/2017/nr/c7nr03153j#!divAbstract'>Cherukara et al.</a>",
		"abstract": "As simulations grow in scale, the need for in situ analysis methods to handle the large data produced grows correspondingly. One desirable approach to in situ visualization is in transit visualization. By decoupling the simulation and visualization code, in transit approaches alleviate common difficulties with regard to the scalability of the analysis, ease of integration, usability, and impact on the simulation. We present libIS, a lightweight, flexible library which lowers the bar for using in transit visualization. Our library works on the concept of abstract regions of space containing data, which are transferred from the simulation to the visualization clients upon request, using a client-server model. We also provide a SENSEI analysis adaptor, which allows for transparent deployment of in transit visualization. We demonstrate the flexibility of our approach on batch analysis and interactive visualization use cases on different HPC resources.",
		"downloads": [
			{
				"title": "Code",
				"list": [
					{
						"title": "libIS",
						"link": "https://github.com/ospray/libIS",
						"icon": "fab fa-github"
					},
					{
						"title": "OSPRay-based Viewer Application",
						"link": "https://github.com/ospray/ospray_senpai",
						"icon": "fab fa-github"
					},
					{
						"title": "SENSEI Adaptor for libIS and LAMMPS",
						"link": "https://gitlab.kitware.com/sensei/sensei/tree/lammps/miniapps/lammps",
						"icon": "fab fa-gitlab"
					},
					{
						"title": "Theta SSH Tunneling Script",
						"link": "https://github.com/Twinklebear/theta-tunnel",
						"icon": "fab fa-github"
					}
				]
			}
		]
	},
	{
		"title": "VisIt-OSPRay: Toward an Exascale Volume Visualization System",
		"authors": "Qi Wu, Will Usher, Steve Petruzza, Sidharth Kumar, Feng Wang,
			Ingo Wald, Valerio Pascucci, and Charles D. Hansen",
		"venue": "Eurographics Symposium on Parallel Graphics and Visualization",
		"paper_pdf": "http://sci.utah.edu/~will/papers/visit-ospray-egpgv18.pdf",
		"teaser": "https://i.imgur.com/JgAOmst.png",
		"thumb": "https://i.imgur.com/RxCFTxo.jpg",
		"year": 2018,
		"short_title": "visit-ospray",
		"doi": "10.2312/pgv.20181091",
		"bibtex": "@inproceedings{Wu_VisItOSPRay_2018,\n
			booktitle = {Eurographics Symposium on Parallel Graphics and Visualization},\n
			editor = {Hank Childs and Fernando Cucchietti},\n
			title = {{VisIt}-{OSPRay}: {Toward} an {Exascale} {Volume} {Visualization} {System}},\n
			author = {Wu, Qi and Usher, Will and Petruzza, Steve and Kumar, Sidharth and Wang, Feng and Wald, Ingo and Pascucci, Valerio and Hansen, Charles D.},\n
			year = {2018},\n
			publisher = {The Eurographics Association},\n
			ISSN = {1727-348X},\n
			ISBN = {978-3-03868-054-3},\n
			DOI = {10.2312/pgv.20181091}\n
		}",
		"abstract": "Large-scale simulations can easily produce data in excess of what can be efficiently visualized using production visualization software, making it challenging for scientists to gain insights from the results of these simulations. This trend is expected to grow with exascale. To meet this challenge, and run on the highly parallel hardware being deployed on HPC system, rendering systems in production visualization software must be redesigned to perform well at these new scales and levels of parallelism. In this work, we present VisIt-OSPRay, a high-performance, scalable, hybrid-parallel rendering system in VisIt, using OSPRay and IceT, coupled with PIDX for scalable I/O. We examine the scalability and memory efficiency of this system and investigate further areas for improvement to prepare VisIt for upcoming exascale workloads.",
		"teaser_caption": "High-quality interactive volume visualization using VisIt-OSPRay: <b>a)</b> volume rendering of O<sub>2</sub> concentration inside a combustion chamber, data courtesy of the <a href=\"http://ccmsc.sci.utah.edu/\">University of Utah CCMSC</a>; <b>b)</b> volume rendering of the Richtmyer-Meshkov Instability; <b>c)</b> visualization of a supernova simulation; <b>d)</b> visualization of the aneurysm dataset using volume rendering and streamlines; <b>e)</b> scalable volume rendering of the 966GB DNS data on 64 Stampede2 Intel Xeon Phi Knight's Landing nodes."
	},
	{
		"title": "Scalable Data Management of the Uintah Simulation Framework for Next-Generation Engineering Problems with Radiation",
		"authors": "Sidharth Kumar, Alan Humphrey, Will Usher, Steve Petruzza,
				Brad Peterson, John A. Schmidt, Derek Harris, Ben Isaac,
				Jeremy Thornock, Todd Harman, Valerio Pascucci, and Martin Berzins",
		"venue": "Supercomputing Frontiers",
		"paper_pdf": "http://sci.utah.edu/~will/papers/scasia18.pdf",
		"thumb": "https://i.imgur.com/w5YWvd6.png",
		"year": 2018,
		"short_title": "scasia18",
		"doi": "10.1007/978-3-319-69953-0_13",
		"downloads": [
			{
				"title": "Videos",
				"list": [
					{
						"title": "Uintah UASC Coal Boiler Visualization",
						"link": "https://youtu.be/vpJtHTzArq4"
					}
				]
			}
		],
		"bibtex": "@incollection{kumar_scalable_2018,\n
			title = {Scalable {Data} {Management} of the {Uintah} {Simulation} {Framework} for {Next}-{Generation} {Engineering} {Problems} with {Radiation}},\n
			volume = {10776},\n
			isbn = {978-3-319-69952-3 978-3-319-69953-0},\n
			booktitle = {Supercomputing {Frontiers}},\n
			publisher = {Springer International Publishing},\n
			author = {Kumar, Sidharth and Humphrey, Alan and Usher, Will and Petruzza, Steve and Peterson, Brad and Schmidt, John A. and Harris, Derek and Isaac, Ben and Thornock, Jeremy and Harman, Todd and Pascucci, Valerio and Berzins, Martin},\n
			editor = {Yokota, Rio and Wu, Weigang},\n
			year = {2018},\n
			doi = {10.1007/978-3-319-69953-0_13},\n
			pages = {219--240},\n}",
			"abstract": "The need to scale next-generation industrial engineering
				problems to the largest computational platforms presents unique challenges.
				This paper focuses on data management related problems faced
				by the Uintah simulation framework at a production scale of 260K processes.
				Uintah provides a highly scalable asynchronous many-task runtime
				system, which in this work is used for the modeling of a 1000
				megawatt electric (MWe) ultra-supercritical (USC) coal boiler. At 260K
				processes, we faced both parallel I/O and visualization related challenges,
				e.g., the default file-per-process I/O approach of Uintah did not scale
				on Mira. In this paper we present a simple to implement, restructuring
				based parallel I/O technique. We impose a restructuring step that
				alters the distribution of data among processes. The goal is to distribute
				the dataset such that each process holds a larger chunk of data, which is
				then written to a file independently. This approach finds a middle ground
				between two of the most common parallel I/O schemes–file per process
				I/O and shared file I/O–in terms of both the total number of generated
				files, and the extent of communication involved during the data aggregation
				phase. To address scalability issues when visualizing the simulation
				data, we developed a lightweight renderer using OSPRay, which allows
				scientists to visualize the data interactively at high quality and make
				production movies. Finally, this work presents a highly efficient and scalable
				radiation model based on the sweeping method, which significantly
				outperforms previous approaches in Uintah, like discrete ordinates. The
				integrated approach allowed the USC boiler problem to run on 260K
				CPU cores on Mira."
	},
	{
		"title": "CPU Volume Rendering of Adaptive Mesh Refinement Data",
		"authors": "Ingo Wald, Carson Brownlee, Will Usher, Aaron Knoll",
		"venue": "SIGGRAPH Asia 2017 Symposium on Visualization",
		"paper_pdf": "http://sci.utah.edu/~will/papers/cvamr/cvamr.pdf",
		"teaser": "https://i.imgur.com/CqZc3VJ.png",
		"thumb": "https://i.imgur.com/JFShB4G.png",
		"year": 2017,
		"short_title": "cvamr",
		"doi": "10.1145/3139295.3139305",
		"bibtex": "@inproceedings{Wald_CVAMR_2017,\n
			author = {Wald, Ingo and Brownlee, Carson and Usher, Will and Knoll, Aaron},\n
			title = {CPU Volume Rendering of Adaptive Mesh Refinement Data},\n
			booktitle = {SIGGRAPH Asia 2017 Symposium on Visualization},\n
			series = {SA '17},\n
			year = {2017},\n
			isbn = {978-1-4503-5411-0},\n
			location = {Bangkok, Thailand},\n
			pages = {9:1--9:8},\n
			articleno = {9},\n
			numpages = {8},\n
			url = {http://doi.acm.org/10.1145/3139295.3139305},\n
			doi = {10.1145/3139295.3139305},\n
			acmid = {3139305},\n
			publisher = {ACM},\n
			address = {New York, NY, USA},\n
		}",
		"abstract": "Adaptive Mesh Refinement (AMR) methods are widespread
		in scientific computing, and visualizing the resulting data with
		efficient and accurate rendering methods can be vital for enabling 
		interactive data exploration.
		In this work, we
		detail a comprehensive solution for directly volume rendering block-structured 
		(Berger-Colella) AMR data in the
		OSPRay interactive CPU ray tracing framework. In particular, we
		contribute a general method for representing and traversing AMR data
		using a kd-tree structure, and four different reconstruction
		options, one of which in particular (the basis function approach)
		is novel compared to existing methods. We demonstrate our system on two
		types of block-structured AMR data and compressed scalar
		field data, and show how it can be easily used in existing production-ready
		applications through a prototypical integration in the widely used visualization program ParaView.",
		"teaser_caption": "Two examples of our method (integrated within the OSPRay ray tracer):
		Left: 1.8GB Cosmos AMR data, rendered in ParaView. Right: a 57GB NASA Chombo simulation,
		rendered with ambient occlusion and shadows alongside mesh geometry."
	},
	{
		"title": "Progressive CPU Volume Rendering with Sample Accumulation",
		"authors": "Will Usher, Jefferson Amstutz, Carson Brownlee, Aaron Knoll, Ingo Wald",
		"venue": "Eurographics Symposium on Parallel Graphics and Visualization",
		"paper_pdf": "http://sci.utah.edu/~will/papers/savr/savr.pdf",
		"teaser": "https://i.imgur.com/15y1f8I.png",
		"thumb": "https://i.imgur.com/tdxjYs3.png",
		"short_title": "savr",
		"doi": "10.2312/pgv.20171090",
		"year": 2017,
		"bibtex": "@inproceedings{Usher_SAVR_2017,\n
			booktitle={Eurographics Symposium on Parallel Graphics and Visualization},\n
			editor={Alexandru Telea and Janine Bennett},\n
			title={{Progressive CPU Volume Rendering with Sample Accumulation}},\n
			author={Usher, Will and Amstutz, Jefferson and Brownlee, Carson and Knoll, Aaron and Wald, Ingo},\n
			year={2017},\n
			publisher={The Eurographics Association},\n
			issn={1727-348X},\n
			isbn={978-3-03868-034-5},\n
			doi={10.2312/pgv.20171090},\n}",
		"abstract": "We present a new method for progressive volume rendering by accumulating object-space samples over successively rendered frames. Existing methods for progressive refinement either use image space methods or average pixels over frames, which can blur features or integrate incorrectly with respect to depth. Our approach stores samples along each ray, accumulates new samples each frame into a buffer, and progressively interleaves and integrates these samples. Though this process requires additional memory, it ensures interactivity and is well suited for CPU architectures with large memory and cache. This approach also extends well to distributed rendering in cluster environments. We implement this technique in Intel’s open source OSPRay CPU ray tracing framework and demonstrate that it is particularly useful for rendering volumetric data with costly sampling functions.",
		"teaser_caption": "(a-c) Progressive refinement with Sample-Accumulation Volume Rendering (SAVR) on the 40GB Landing Gear AMR dataset using a prototype AMR sampler. The SAVR algorithm correctly accumulates frames to progressively refine the image. After 16 frames of accumulation the volume is sampled at the Nyquist limit, with some small noise, by 32 frames the noise has been removed. SAVR extends to distributed data, in (d) we show the 1TB DNS dataset, a 10240×7680×1536 uniform grid, rendered interactively across 64 second-generation Intel Xeon Phi \"Knights Landing\" (KNL) processor nodes on Stampede 1.5 at a 6144×1024 resolution. While interacting, our method achieves around 5.73 FPS.",
		"downloads": [
			{
				"title": "Code",
				"list": [
					{
						"title": "SAVR module for OSPRay",
						"link": "https://github.com/ospray/module_savr",
						"icon": "fab fa-github"
					}
				]
			}
		]
	},
	{
		"title": "In Situ Exploration of Particle Simulations with CPU Ray Tracing",
		"authors": "Will Usher, Ingo Wald, Aaron Knoll, Michael Papka, Valerio Pascucci",
		"venue": "Supercomputing Frontiers and Innovations",
		"paper_pdf": "http://sci.utah.edu/~will/papers/in_situ_particles/in_situ_particles.pdf",
		"teaser": "https://i.imgur.com/DO3JqOb.png",
		"thumb": "https://i.imgur.com/gieTAy3.png",
		"short_title": "isp-jsfi",
		"doi": "10.14529/jsfi160401",
		"year": 2016,
		"bibtex": "@article{Usher_InSituParticles_2016,\n
			author={Will Usher and Ingo Wald and Aaron Knoll and Michael Papka and Valerio Pascucci},\n
			title={In {Situ} {Exploration} of {Particle} {Simulations} with {CPU} {Ray} {Tracing}},\n
			journal={{Supercomputing} {Frontiers} and {Innovations}},\n
			volume={3},\n
			number={4},\n
			year={2016},\n
			issn={2313-8734},\n
			doi={10.14529/jsfi160401},\n}",
		"abstract": "We present a system for interactive in situ visualization of large particle simulations, suitable for general CPU-based HPC architectures. As simulations grow in scale, in situ methods are needed to alleviate IO bottlenecks and visualize data at full spatio-temporal resolution. We use a lightweight loosely-coupled layer serving distributed data from the simulation to a data-parallel renderer running in separate processes. Leveraging the OSPRay ray tracing framework for visualization and balanced P-k-d trees, we can render simulation data in real-time, as they arrive, with negligible memory overhead. This flexible solution allows users to perform exploratory in situ visualization on the same computational resources as the simulation code, on dedicated visualization clusters or remote workstations, via a standalone rendering client that can be connected or disconnected as needed. We evaluate this system on simulations with up to 227M particles in the LAMMPS and Uintah computational frameworks, and show that our approach provides many of the advantages of tightly-coupled systems, with the flexibility to render on a wide variety of remote and co-processing resources.",
		"teaser_caption": "A coal particle combustion simulation in Uintah at three different timesteps with (left to right): 34.61M, 48.46M and 55.39M particles, with attribute based culling showing the full jet (top) and the front in detail (bottom). Using our in situ library to query and send data to our rendering client in OSPRay these images are rendered interactively with ambient occlusion, averaging around 13 FPS at 1920×1080. The renderer is run on 12 nodes of the Stampede supercomputer and pulls data from a Uintah simulation running on 64 processes (4 nodes). Our loosely-coupled in situ approach allows for live exploration at the full temporal fidelity of the simulation, without prohibitive IO cost.",
		"downloads": [
			{
				"title": "Code",
				"list": [
					{
						"title": "In Situ Particles module for OSPRay",
						"link": "https://github.com/Twinklebear/in-situ-particles",
						"icon": "fab fa-github"
					}
				]
			}
		]
	},
	{
		"title": "VTK-m: Accelerating the Visualization Toolkit for Massively Threaded Architectures",
		"authors": "Kenneth Moreland, Christopher Sewell, William Usher, Li-ta Lo, Jeremy Meredith,
				David Pugmire, James Kress, Hendrik Schroots, Kwan-Liu Ma, Hank Childs, Matthew Larsen,
				Chun-Ming Chen, Robert Maynard, Berk Geveci",
		"venue": "IEEE Computer Graphics and Applications",
		"paper_pdf": "http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7466740",
		"doi": "10.1109/MCG.2016.48",
		"year": 2016,
		"short_title": "vtkm",
		"bibtex": "@article{Moreland_VTKm_2016,\n
			author={K. Moreland and C. Sewell and W. Usher and L. t. Lo and J. Meredith and D. Pugmire and J. Kress and H. Schroots and K. L. Ma and H. Childs and M. Larsen and C. M. Chen and R. Maynard and B. Geveci},\n
			journal={{IEEE} {Computer} {Graphics} and {Applications}},\n
			title={{VTK-m}: {Accelerating} the {Visualization} {Toolkit} for {Massively} {Threaded} {Architectures}},\n
			year={2016},\n
			volume={36},\n
			number={3},\n
			pages={48-58},\n
			doi={10.1109/MCG.2016.48},\n
			ISSN={0272-1716},\n
			month={May},\n}",
		"abstract": "One of the most critical challenges for high-performance computing (HPC) scientific visualization is execution on massively threaded processors. Of the many fundamental changes we are seeing in HPC systems, one of the most profound is a reliance on new processor types optimized for execution bandwidth over latency hiding. Our current production scientific visualization software is not designed for these new types of architectures. To address this issue, the VTK-m framework serves as a container for algorithms, provides flexible data representation, and simplifies the design of visualization algorithms on new and future computer architecture.",
		"downloads": [
			{
				"title": "Code",
				"list": [
					{
						"title": "VTK-m",
						"link": "https://gitlab.kitware.com/vtk/vtk-m",
						"icon": "fab fa-gitlab"
					}
				]
			}
		]
	},
	{
		"title": "CPU Ray Tracing Large Particle Data with Balanced P-k-d Trees",
		"authors": "Ingo Wald, Aaron Knoll, Gregory P. Johnson, Will Usher, Valerio Pasucci, Michael E. Papka",
		"venue": "IEEE Vis (conference)",
		"paper_pdf": "http://sci.utah.edu/~will/papers/pkd/pkd_tree.pdf",
		"doi": "10.1109/SciVis.2015.7429492",
		"teaser": "https://i.imgur.com/1YNpRJ1.png",
		"thumb": "https://i.imgur.com/qpTN5kR.png",
		"short_title": "pkd",
		"year": 2015,
		"bibtex": "@inproceedings{Wald_PKD_2015,\n
			author={I. Wald and A. Knoll and G. P. Johnson and W. Usher and V. Pascucci and M. E. Papka},\n
			booktitle={2015 IEEE Scientific Visualization Conference (SciVis)},\n
			title={{CPU} ray tracing large particle data with balanced {P-k-d} trees},\n
			year={2015},\n
			pages={57-64},\n
			doi={10.1109/SciVis.2015.7429492},\n
			month={Oct},\n}",
		"abstract": "We present a novel approach to rendering large particle data sets from molecular dynamics, astrophysics and other sources. We employ a new data structure adapted from the original balanced k-d tree, which allows for representation of data with trivial or no overhead. In the OSPRay visualization framework, we have developed an efficient CPU algorithm for traversing, classifying and ray tracing these data. Our approach is able to render up to billions of particles on a typical workstation, purely on the CPU, without any approximations or level-of-detail techniques, and optionally with attribute-based color mapping, dynamic range query, and advanced lighting models such as ambient occlusion and path tracing.",
		"teaser_caption": "Full-detail ray tracing of giga-particle data sets. From left to right: CosmicWeb early universe data set from a P3D simulation with 29 billion particles; a 100 million atom molecular dynamics Al<sub>2</sub>O<sub>3</sub>−SiC materials fracture simulation; and a 1.3 billion particle Uintah MPM detonation simulation. Using a quad-socket, 72-core 2.5GHz Intel Xeon E7-8890 v3 Processor with 3TB RAM and path-tracing with progressive refinement at 1 sample per pixel, these far and close images (above and below) are rendered at 1.6 (far) / 1.0 (close) FPS (left), 2.0 / 1.2 FPS (center), and 1.0 / 0.9 FPS (right), respectively, at 4K (3840×2160) resolution. All examples use our balanced P-k-d tree, an acceleration structure which requires little or no memory cost beyond the original data.",
		"downloads": [
			{
				"title": "Code",
				"list": [
					{
						"title": "PKD module for OSPRay",
						"link": "https://github.com/ingowald/ospray-module-pkd",
						"icon": "fab fa-github"
					}
				]
			}
		]
	}
]

