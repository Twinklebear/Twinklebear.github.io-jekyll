[
    {
		"title": "Interactive Visualization of Terascale Data in the Browser: Fact or Fiction?",
		"authors": "Will Usher and Valerio Pascucci",
		"venue": "IEEE Symposium on Large Data Analysis and Visualization (LDAV)",
		"paper_pdf": "http://sci.utah.edu/~will/papers/teraweb-ldav20.pdf",
		"teaser": "https://i.imgur.com/XmBXWgu.png",
		"thumb": "https://i.imgur.com/I2FBGOI.png",
		"year": 2020,
		"exact_date": "2020-10-25 00:00:00 -0600",
		"short_title": "teraweb",
        "award": "Honorable Mention",
        "selected": true,
		"bibtex": "@inproceedings{usher_teraweb_2020,\n
			booktitle = {10th IEEE Symposium on Large Data Analysis and Visualization},\n
			title = {{Interactive Visualization of Terascale Data in the Browser: Fact or Fiction?}},\n
			author = {Usher, Will and Pascucci, Valerio},\n
			year = {2020},\n}",
        "abstract": "Information visualization applications have become ubiquitous, in no
            small part thanks to the ease of wide distribution and deployment to
            users enabled by the web browser. Scientific visualization applications,
            relying on native code libraries and parallel processing,
            have been less suited to such widespread distribution, as browsers
            do not provide the required libraries or compute capabilities.
            In this paper, we revisit this gap in visualization technologies and explore
            how new web technologies, WebAssembly and WebGPU,
            can be used to deploy powerful visualization solutions 
            for large-scale scientific data in the browser.
            In particular, we evaluate the programming effort required to bring
            scientific visualization
            applications to the browser through these technologies and assess
            their competitiveness against classic native solutions.
            As a main example, we present a new GPU-driven isosurface
            extraction method for block-compressed data sets,
            that is suitable for interactive isosurface computation
            on large volumes in resource-constrained environments, such as the browser.
            We conclude that web browsers are on the verge of becoming a
            competitive platform for even the most demanding scientific visualization
            tasks, such as interactive visualization of isosurfaces from a 1TB DNS simulation.
            We call on researchers and developers to consider investing
            in a community software stack to ease use of these upcoming browser
            features to bring accessible scientific visualization to the browser.",
        "teaser_caption": "Interactive visualization of an isosurface of a ~1TB dataset entirely in the web browser.
            The full data is a float64 10240×7680×1536 grid computed by a DNS simulation.
            The isosurface is interactively computed and visualized entirely in the browser using our GPU isosurface
            computation algorithm for block-compressed data, 
            after applying advanced precision and resolution trade-offs.
            The surface consists of 137.5M triangles and is computed in 526ms on an RTX 2070 using WebGPU in Chrome
            and rendered at 30FPS at 1280×720.
            The original surface, shown in the right split image, consists of 4.3B triangles and was computed
            with VTK's Flying Edges filter on a quad-socket Xeon server in 78s using 1.3TB of memory.",
		"supplemental_video": "https://www.youtube.com/embed/O7Tboj2dDVA",
		"presentation_video": "https://www.youtube.com/embed/piBNlAZKHAM?start=1000",
		"downloads": [
			{
				"title": "Code",
				"list": [
					{
						"title": "WebGPU Block Compressed Marching Cubes",
						"link": "https://github.com/Twinklebear/webgpu-bcmc",
						"icon": "fab fa-github"
					}
				]
			}
		]
	},
    {
		"title": "Improving the Usability of Virtual Reality Neuron Tracing with Topological Elements",
		"authors": "Torin McDonald, Will Usher, Nate Morrical, Attila Gyulassy, Steve Petruzza,
            Frederick Federer, Alessandra Angelucci, and Valerio Pascucci",
		"venue": "IEEE Transactions on Visualization and Computer Graphics",
		"paper_pdf": "http://sci.utah.edu/~will/papers/mscvrnt-vis20.pdf",
		"teaser": "https://i.imgur.com/NPqNeOU.png",
		"thumb": "https://i.imgur.com/UEFdOAt.png",
		"year": 2021,
		"exact_date": "2020-10-27 00:00:00 -0600",
		"short_title": "mscvrnt",
        "selected": true,
		"bibtex": "@article{mcdonald_mscvrnt_2021,\n
			author={McDonald, Torin and Usher, Will and Morrical, Nate and Gyulassy, Attila and Petruzza, Steve
                and Federer, Frederick and Angelucci, Alessandra and Pascucci, Valerio},\n
			journal={{IEEE} {Transactions} on {Visualization} and {Computer} {Graphics}},\n
			title={{Improving} the {Usability} of {Virtual} {Reality} {Neuron} {Tracing} with {Topological} {Elements}},\n
			year={2021},\n}",
        "abstract": "Researchers in the field of connectomics are working to reconstruct a map of neural
            connections in the brain in order to understand at a fundamental level how the brain processes information.
            Constructing this wiring diagram is done by tracing neurons through high-resolution image stacks acquired
            with fluorescence microscopy imaging techniques. While a large number of automatic tracing algorithms have
            been proposed, these frequently rely on local features in the data and fail on noisy data or ambiguous cases,
            requiring time-consuming manual correction. As a result, manual and semi-automatic tracing methods remain the
            state-of-the-art for creating accurate neuron reconstructions. We propose a new semi-automatic method that uses
            topological features to guide users in tracing neurons and integrate this method within a virtual reality (VR)
            framework previously used for manual tracing. Our approach augments both visualization and interaction with
            topological elements, allowing rapid understanding and tracing of complex morphologies. In our pilot study,
            neuroscientists demonstrated a strong preference for using our tool over prior approaches, reported less
            fatigue during tracing, and commended the ability to better understand possible paths and alternatives.
            Quantitative evaluation of the traces reveals that users' tracing speed increased, while retaining similar
            accuracy compared to a fully manual approach.",
        "teaser_caption": "Left to right: A connected graph of ridge-like structures is extracted from the Morse-Smale complex (MSC), containing a superset
            of the possible neuron segments in the data. Our MSC-guided semi-automatic tracing tool enables users to rapidly trace paths and
            view a live preview as they do so (orange line). When satisfied with the trace, they can add it to the reconstruction (white line).",
		"supplemental_video": "https://www.youtube.com/embed/X0JDCms3gKk",
		"presentation_video": "https://www.youtube.com/embed/GVfO0F-4T7g?start=2494",
	},
    {
		"title": "Ray Tracing Structured AMR Data Using ExaBricks",
		"authors": "Ingo Wald, Stefan Zellmann, Will Usher, Nate Morrical, Ulrich Lang, and Valerio Pascucci",
		"venue": "IEEE Transactions on Visualization and Computer Graphics",
		"paper_pdf": "http://sci.utah.edu/~will/papers/exabrick-vis20.pdf",
		"teaser": "https://i.imgur.com/9upElMb.jpg",
		"thumb": "https://i.imgur.com/4Ghd7BO.jpg",
		"year": 2021,
		"exact_date": "2020-10-27 00:00:00 -0600",
		"short_title": "exabrick",
        "selected": true,
		"bibtex": "@article{wald_exabrick_2021,\n
			author={Wald, Ingo and Zellmann, Stefan and Usher, Will and Morrical, Nate
                and Lang, Ulrich and Pascucci, Valerio},\n
			journal={{IEEE} {Transactions} on {Visualization} and {Computer} {Graphics}},\n
			title={{Ray} {Tracing} {Structured} {AMR} {Data} {Using} {ExaBricks}},\n
			year={2021},\n}",
        "abstract": "Structured Adaptive Mesh Refinement (Structured AMR) enables simulations to adapt
            the domain resolution to save computation and storage, and has become one of the dominant
            data representations used by scientific simulations; however, efficiently rendering such
            data remains a challenge. We present an efficient approach for volume- and iso-surface ray
            tracing of Structured AMR data on GPU-equipped workstations, using a combination of two different
            data structures. Together, these data structures allow a ray tracing based renderer to quickly
            determine which segments along the ray need to be integrated and at what frequency, while also
            providing quick access to all data values required for a smooth sample reconstruction kernel.
            Our method makes use of the RTX ray tracing hardware for surface rendering, ray marching, space skipping,
            and adaptive sampling; and allows for interactive changes to the transfer function and implicit
            iso-surfacing thresholds. We demonstrate that our method achieves high performance with little
            memory overhead, enabling interactive high quality rendering of complex AMR data sets on
            individual GPU workstation.",
        "teaser_caption": "The Exajet contains an AMR simulation of air flow around the left side of a plane,
            and consists of 656M cells (across four refinement levels) plus 63.2M triangles. For rendering we
            mirror the data set via instancing, resulting in effectively 1.31B instanced cells and 126M instanced
            triangles. This visualization—rendered with our method—shows flow vorticity and velocity, with an
            implicitly ray-traced iso-surface of the vorticity (color-mapped by velocity), plus volume ray tracing of
            the vorticity field. At a resolution of 2500×625, and running on a workstation with two RTX 8000 GPUs,
            this configuration renders in roughly 252 milliseconds.",
		"supplemental_video": "https://www.youtube.com/embed/r8G2qECeoqU",
		"presentation_video": "https://www.youtube.com/embed/s6CZzP6zT7s?start=3183"
	},
    {
		"title": "Efficient and Flexible Hierarchical Data Layouts for a Unified Encoding of Scalar Field Precision and Resolution",
		"authors": "Duong Hoang, Brian Summa, Harsh Bhatia, Peter Lindstrom,
            Pavol Klacansky, Will Usher, Peer-Timo Bremer and Valerio Pascucci",
		"venue": "IEEE Transactions on Visualization and Computer Graphics",
		"paper_pdf": "http://sci.utah.edu/~will/papers/varprec-vis20.pdf",
		"teaser": "https://i.imgur.com/UICjif7.png",
		"thumb": "https://i.imgur.com/jzIIRBX.jpg",
		"year": 2021,
		"exact_date": "2020-10-27 00:00:00 -0600",
		"short_title": "varprec-vis20",
        "selected": true,
		"bibtex": "@article{hoang_efficient_2021,\n
			author={Hoang, Duong and Summa, Brian and Bhatia, Harsh and Lindstrom, Peter and
                Klacansky, Pavol and Usher, Will and Bremer, Peer-Timo and Pascucci, Valerio},\n
			journal={{IEEE} {Transactions} on {Visualization} and {Computer} {Graphics}},\n
			title={{Efficient} and {Flexible} {Hierarchical} {Data} {Layouts} for a
                {Unified} {Encoding} of {Scalar} {Field} {Precision} and {Resolution}},\n
			year={2021},\n}",
        "abstract": "To address the problem of ever-growing scientific data sizes making data movement
            a major hindrance to analysis, we introduce a novel encoding for scalar fields: a unified tree
            of resolution and precision, specifically constructed so that valid cuts correspond to sensible
            approximations of the original field in the precision-resolution space. Furthermore, we introduce
            a highly flexible encoding of such trees that forms a parameterized family of data hierarchies.
            We discuss how different parameter choices lead to different trade-offs in practice, and show
            how specific choices result in known data representation schemes such as ZFP, IDX, and JPEG2000.
            Finally, we provide system-level details and empirical evidence on how such hierarchies
            facilitate common approximate queries with minimal data movement and time, using real-world data
            sets ranging from a few gigabytes to nearlya terabyte in size. Experiments suggest that our new
            strategy of combining reductions in resolution and precision is competitive with state-of-the-art
            compression techniques with respect to data quality, while being significantly more flexible and
            orders of magnitude faster, and requiring significantly reduced resources",
        "teaser_caption": "We propose a hierarchical data layout that allows for various forms of progressive
            decoding that modulate improvements in both precision and resolution. Each progressive decoding
            traces a monotonic nondecreasing curve in the precision-resolution space from the origin, 0%,
            to the full data, 100% (shown in (a)). Using a 900 GB turbulent channel flow field (10240×7680×1536, float64)
            (b), we demonstrate three approximations (c,d,e) of progressively increasing quality decoded along the
            curve in (a). The time to decode the data and RAM used are shown in the figure; data retrieved
            values are inclusive of the preceding points along the curve",
		"presentation_video": "https://www.youtube.com/embed/gSQglICXNd0?start=5950",
	},
    {
		"title": "A Virtual Frame Buffer Abstraction for Parallel Rendering of Large Tiled Display Walls",
		"authors": "Mengjiao Han, Ingo Wald, Will Usher, Nate Morrical, Aaron Knoll, Valerio Pascucci
            and Chris R. Johnson",
		"venue": "IEEE VIS Short Papers",
		"paper_pdf": "http://sci.utah.edu/~will/papers/ospDisplayWall-short.pdf",
		"teaser": "https://i.imgur.com/e4HJywq.jpg",
		"thumb": "https://i.imgur.com/AvS0RGC.jpg",
		"year": 2020,
		"exact_date": "2020-10-27 00:00:00 -0600",
		"short_title": "ospdisplaywall",
        "selected": false,
		"bibtex": "@inproceedings{han_displaywall_2020,\n
			author={Han, Mengjiao and Wald, Ingo and Usher, Will and Morrical, Nate and Knoll, Aaron
                and Pascucci, Valerio and Johnson, Chris R.},\n
			booktitle={IEEE VIS 2020 - Short Papers},\n
			title={A {Virtual} {Frame} {Buffer} {Abstraction} for {Parallel} {Rendering} of {Large} {Tiled} {Display} {Walls}},\n
			year={2020},\n}",
        "abstract": "We present dw2, a flexible and easy-to-use software
            infrastructure for interactive rendering of large tiled display
            walls. Our library represents the tiled display wall as a single
            virtual screen through a display 'service', which renderers connect
            to and send image tiles to be displayed, either from an on-site or
            remote cluster. The display service can be easily configured to
            support a range of typical network and display hardware configurations;
            the client library provides a straightforward interface for easy integration
            into existing renderers. We evaluate the performance of our display wall
            service in different configurations using a CPU and GPU ray tracer,
            in both on-site and remote rendering scenarios using multiple display walls.",
        "teaser_caption": "Left: The Disney Moana Island rendered remotely with OSPRay's
            path tracer at full detail using 128 Skylake Xeon (SKX) nodes on Stampede2
            and streamed to the 132Mpixel POWERwall display wall, averages 0.2-1.2 FPS.
            Right: The Boeing 777 model, consisting of 349M triangles, rendered remotely with
            OSPRay's scivis renderer using 64 Intel Xeon Phi Knight’s Landing nodes on
            Stampede2 and streamed to the POWERwall, averages 6-7 FPS",
		"supplemental_video": "https://www.youtube.com/embed/tXgYoaf3NBU",
		"presentation_video": "https://www.youtube.com/embed/yxHYxo2rT8c?start=3192",
		"downloads": [
			{
				"title": "Code",
				"list": [
					{
						"title": "Display Wall 2",
						"link": "https://github.com/MengjiaoH/display_wall",
						"icon": "fab fa-github"
					}
				]
			}
		]
	},
    {
		"title": "A terminology for in situ visualization and analysis systems",
		"authors": "Hank Childs, Sean D. Ahern, James Ahrens, Andrew C. Bauer, Janine Bennett,
            E. Wes Bethel, Peer-Timo Bremer, Eric Brugger, Joseph Cottam, Matthieu Dorier,
            Soumya Dutta, Jean M. Favre, Thomas Fogal, Steffen Frey, Christoph Garth, Berk Geveci,
            William F. Godoy, Charles D. Hansen, Cyrus Harrison, Bernd Hentschel, Joseph Insley,
            Chris R. Johnson, Scott Klasky, Aaron Knoll, James Kress, Matthew Larsen, Jay Lofstead,
            Kwan-Liu Ma, Preeti Malakar, Jeremy Meredith, Kenneth Moreland, Paul Navrátil, Patrick O'Leary,
            Manish Parashar, Valerio Pascucci, John Patchett, Tom Peterka, Steve Petruzza,
            Norbert Podhorszki, David Pugmire, Michel Rasquin, Silvio Rizzi, David H. Rogers,
            Sudhanshu Sane, Franz Sauer, Robert Sisneros, Han-Wei Shen, Will Usher, Rhonda Vickery,
            Venkatram Vishwanath, Ingo Wald, Ruonan Wang, Gunther H. Weber, Brad Whitlock, Matthew Wolf, Hongfeng Yu, Sean B. Ziegeler",
		"venue": "The International Journal of High Performance Computing Applications",
		"paper_pdf": "http://sci.utah.edu/~will/papers/istp.pdf",
		"year": 2020,
		"exact_date": "2020-08-14 00:00:00 -0600",
		"short_title": "istp",
        "selected": false,
		"bibtex": "@article{childs_istp_20,\n
            author={Hank Childs and Sean D. Ahern and James Ahrens and Andrew C. Bauer and
                Janine Bennett and E. Wes Bethel and Peer-Timo Bremer and Eric Brugger and
                Joseph Cottam and Matthieu Dorier and Soumya Dutta and Jean M. Favre and
                Thomas Fogal and Steffen Frey and Christoph Garth and Berk Geveci and William F. Godoy and
                Charles D. Hansen and Cyrus Harrison and Bernd Hentschel and Joseph Insley and
                Chris R. Johnson and Scott Klasky and Aaron Knoll and James Kress and Matthew Larsen and
                Jay Lofstead and Kwan-Liu Ma and Preeti Malakar and Jeremy Meredith and Kenneth Moreland and
                Paul Navrátil and Patrick O'Leary and Manish Parashar and Valerio Pascucci and John Patchett and
                Tom Peterka and Steve Petruzza and Norbert Podhorszki and David Pugmire and Michel Rasquin and
                Silvio Rizzi and David H. Rogers and Sudhanshu Sane and Franz Sauer and Robert Sisneros and
                Han-Wei Shen and Will Usher and Rhonda Vickery and Venkatram Vishwanath and Ingo Wald and
                Ruonan Wang and Gunther H. Weber and Brad Whitlock and Matthew Wolf and Hongfeng Yu and Sean B. Ziegeler},
            title={A terminology for in situ visualization and analysis systems},\n
            journal={The International Journal of High Performance Computing Applications}\n}",
        "doi": "10.1177/1094342020935991",
        "abstract": "The term 'in situ processing' has evolved over the last decade to mean
            both a specific strategy for visualizing and analyzing data and an umbrella term for a
            processing paradigm. The resulting confusion makes it difficult for visualization and
            analysis scientists to communicate with each other and with their stakeholders. To address
            this problem, a group of over 50 experts convened with the goal of standardizing terminology.
            This paper summarizes their findings and proposes a new terminology for describing in situ
            systems. An important finding from this group was that in situ systems are best described via
            multiple, distinct axes: integration type, proximity, access, division of execution, operation
            controls, and output type. This paper discusses these axes, evaluates existing systems within the axes,
            and explores how currently used terms relate to the axes."
	},
    {
		"title": "Using Hardware Ray Transforms to Accelerate Ray/Primitive Intersections for Long, Thin Primitive Types",
		"authors": "Ingo Wald, Nate Morrical, Stefan Zellmann, Lei Ma, Will Usher, Tiejun Huang, Valerio Pascucci",
		"venue": "Proceedings of the ACM on Computer Graphics and Interactive Techniques (Proceedings of High Performance Graphics)",
		"paper_pdf": "http://sci.utah.edu/~will/papers/owl-tubes-hpg20.pdf",
		"teaser": "https://i.imgur.com/trtSPND.jpg",
		"thumb": "https://i.imgur.com/PqZhtFN.jpg",
		"year": 2020,
		"exact_date": "2020-07-05 00:00:00 -0600",
		"short_title": "owltubes",
		"presentation_video": "https://www.youtube.com/embed/tYP9pWT51MA",
        "selected": true,
		"bibtex": "@article{wald_owltubes_20,\n
			journal = {Proceedings of the ACM on Computer Graphics and Interactive Techniques (Proceedings of High Performance Graphics)},\n
			title = {{Using Hardware Ray Transforms to Accelerate Ray/Primitive Intersections for Long, Thin Primitive Types}},\n
			author = {Wald, Ingo and Morrical, Nate and Zellmann, Stefan and Ma, Lei and Usher, Will and Huang, Tiejun and Pascucci, Valerio},\n
			year = {2020},\n}",
        "abstract": "With the recent addition of hardware ray tracing capabilities, GPUs have become incredibly efficient
            at ray tracing both triangular geometry, and instances thereof. However, the bounding volume hierarchies
            that current ray tracing hardware relies on are known to struggle with long, thin primitives like cylinders and
            curves, because the axis-aligned bounding boxes that these hierarchies rely on cannot tightly bound such
            primitives. In this paper, we evaluate the use of RTX ray tracing capabilities to accelerate these primitives
            by tricking the GPU's instancing units into executing a hardware-accelerated oriented bounding box (OBB)
            rejection test before calling the user’s intersection program. We show that this can be done with minimal
            changes to the intersection programs and demonstrate speedups of up to 5.9× on a variety of data sets.",
        "teaser_caption": "Three of the models we used for evaluating our method: Left) SciVis2011 contest data set (334.96K
            rounded cylinders via Quilez-style 'capsules', plus 315.88K triangles). Middle,Right) hair/fur on the Blender
            Foundation franck and autumn models (2.4M and 3.4M 'phantom' curve segments, plus 249.62K and 904.40K
            triangles, respectively). For these three models, our method leverages hardware ray transforms to realize
            a hardware-accelerated OBB culling test, achieving speedup of 1.3×, 2.0×, and 2.1×, respectively, over a
            traditional (but also hardware-accelerated) AABB-based BVH (both methods use exactly the same primitive
            intersection codes). Bottom: heat map of number of intersection program evaluations for the two methods,
            respectively."
	},
	{
		"title": "CPU Ray Tracing of Tree-Based Adaptive Mesh Refinement Data",
		"authors": "Feng Wang, Nathan Marshak, Will Usher, Carsten Burstedde, Aaron Knoll, Timo Heister, and Chris R. Johnson",
		"venue": "Computer Graphics Forum",
		"paper_pdf": "http://sci.utah.edu/~will/papers/tamr/tamr.pdf",
		"teaser": "https://i.imgur.com/bwoCE0y.jpg",
		"thumb": "https://i.imgur.com/jpqy4ym.jpg",
		"year": 2020,
		"exact_date": "2020-03-25 00:00:00 -0600",
		"short_title": "tamr",
        "doi": "10.1111/cgf.13958",
		"presentation_video": "https://www.youtube.com/embed/CE20fwd77EY",
        "selected": true,
		"bibtex": "@article{wang_tamr_20,\n
			journal = {Computer Graphics Forum},\n
			title = {{CPU Ray Tracing of Tree-Based Adaptive Mesh Refinement Data}},\n
			author = {Wang, Feng and Marshak, Nathan and Usher, Will and
                Burstedde, Carsten and Knoll, Aaron and Heister, Timo
                and Johson, Chris R.},\n
            DOI = {10.1111/cgf.13958},\n
			year = {2020},\n}",
		"abstract": "Adaptive mesh refinement (AMR) techniques allow for representing a simulation’s computation domain in an adaptive fashion. Although these techniques have found widespread adoption in high-performance computing simulations, visualizing their data output interactively and without cracks or artifacts remains challenging. In this paper, we present an efficient solution for direct volume rendering and hybrid implicit isosurface ray tracing of tree-based AMR (TB-AMR) data. We propose a novel reconstruction strategy, Generalized Trilinear Interpolation (GTI), to interpolate across AMR level boundaries without cracks or discontinuities in the surface normal. We employ a general sparse octree structure supporting a wide range of AMR data, and use it to accelerate volume rendering, hybrid implicit isosurface rendering and value queries. We demonstrate that our approach achieves artifact-free isosurface and volume rendering and provides higher quality output images compared to existing methods at interactive rendering rates.",
		"teaser_caption": "High-fidelity visualization (volume and implicit isosurface rendering) of the NASA ExaJet dataset (field: vorticity). This dataset contains 656M cells (1.31B after instancing) of adaptive resolution and 63.2M triangles (126M after instancing). This 2400×600 image is rendered on a workstation with four Intel Xeon E7-8890 v3 CPUs (72 cores, 2.5 GHz) at a framerate of 6.64 FPS. We show that our system has the capability of ray tracing TB-AMR data in combination with advanced shading effects like ambient occlusion and path tracing.",
		"supplemental_video": "https://www.youtube.com/embed/pVd9Fpv2yoE",
		"downloads": [
			{
				"title": "Supplemental Material",
                "list": [
                    {
                        "title": "Appendix",
                        "link": "http://sci.utah.edu/~will/papers/tamr/tamr-appendix.pdf",
                        "icon": "far fa-file-pdf"
                    },
                    {
                        "title": "SimPy Notebook",
                        "link": "http://sci.utah.edu/~will/papers/tamr/tamr-simpy-notebook.zip",
                        "icon": "far fa-file-archive"
                    },
                    {
                        "title": "OSPRay TB-AMR Module",
                        "link": "https://github.com/ethan0911/TB-AMR",
                        "icon": "fab fa-github"
                    }
                ]
			}
		]
	},
	{
		"title": "A Comparison of Rendering Techniques for 3D Line Sets with Transparency",
		"authors": "Michael Kern, Christoph Neuhauser, Torben Maack, Mengjiao Han,
            Will Usher, and Rüdiger Westermann",
		"venue": "IEEE Transactions on Visualization and Computer Graphics",
		"paper_pdf": "http://sci.utah.edu/~will/papers/tvcg20_oit/tvcg_oit_lines.pdf",
		"teaser": "https://i.imgur.com/Sceg1iM.jpg",
		"thumb": "https://i.imgur.com/Yy9H2ZR.jpg",
		"year": 2020,
		"exact_date": "2020-02-24 00:00:00 -0600",
		"short_title": "tvcg20_oit",
		"doi": "10.1109/TVCG.2020.2975795",
		"bibtex": "@article{kern_comparison_20,\n
			journal = {IEEE Transactions on Visualization and Computer Graphics},\n
			title = {{A Comparison of Rendering Techniques for 3D Line Sets with Transparency}},\n
			author = {Kern, Michael and Neuhauser, Christoph and Maack, Torben and Han, Mengjiao and
                Usher, Will and Westermann, Rüdiger},\n
			year = {2020},\n
			DOI = {10.1109/TVCG.2020.2975795}\n}",
		"abstract": "This paper presents a comprehensive study of rendering techniques for 3D line sets with transparency. The rendering of transparent lines is widely used for visualizing trajectories of tracer particles in flow fields. Transparency is then used to fade out lines deemed unimportant, based on, for instance, geometric properties or attributes defined along with them. Accurate blending of transparent lines requires rendering the lines in back-to-front or front-to-back order, yet enforcing this order for space-filling 3D line sets with extremely high-depth complexity becomes challenging. In this paper, we study CPU and GPU rendering techniques for transparent 3D line sets. We compare accurate and approximate techniques using optimized implementations and several benchmark data sets. We discuss the effects of data size and transparency on quality, performance, and memory consumption. Based on our study, we propose two improvements to per-pixel fragment lists and multi-layer alpha blending. The first improves the rendering speed via an improved GPU sorting operation, and the second improves rendering quality via transparency-based bucketing.",
		"teaser_caption": "Strengths and weaknesses of transparent line rendering techniques. For each pair, the left image shows the ground truth (GT). Right images show (a) approximate blending using MLABDB, (b) opacity over-estimation of MBOIT, (c) reverse blending order of MLABDB, (d) blur effect of MBOIT. Speed-ups to GT rendering technique: (a) 7, (b) 2, (c) 3.5, (d) 4.5.",
		"downloads": [
			{
				"title": "Supplemental Material",
                "list": [
                    {
                        "title": "Appendices",
                        "link": "http://sci.utah.edu/~will/papers/tvcg20_oit/tvcg_oit_lines_appendices.pdf",
                        "icon": "far fa-file-pdf"
                    },
                    {
                        "title": "Benchmark Framework and GPU Rendering Algorithms",
                        "link": "https://github.com/chrismile/PixelSyncOIT",
                        "icon": "fab fa-github"
                    },
                    {
                        "title": "OSPRay Generalized Tubes Module",
                        "link": "https://github.com/MengjiaoH/ospray-module-tubes",
                        "icon": "fab fa-github"
                    },
                    {
                        "title": "Line Data Sets",
                        "link": "http://doi.org/10.5281/zenodo.3637625",
                        "icon": "far fa-file-archive"
                    },
                    {
                        "title": "RTX Ray Tracing Test Framework",
                        "link": "http://doi.org/10.5281/zenodo.3637621",
                        "icon": "far fa-file-archive"
                    }
                ]
			}
		]
	},
	{
		"title": "Scalable Ray Tracing Using the Distributed FrameBuffer",
		"authors": "Will Usher, Ingo Wald, Jefferson Amstutz, Johannes Günther,
			Carson Brownlee, and Valerio Pascucci",
		"venue": "Computer Graphics Forum (Proceedings of EuroVis)",
		"paper_pdf": "http://sci.utah.edu/~will/papers/dfb.pdf",
		"teaser": "https://i.imgur.com/xDw1wI1.jpg",
		"thumb": "https://i.imgur.com/xVd40uT.jpg",
		"year": 2019,
		"exact_date": "2019-07-10 00:00:00 -0600",
		"short_title": "dfb",
		"supplemental_video": "https://www.youtube.com/embed/F2nahk4GAB0",
		"doi": "10.1111/cgf.13702",
        "selected": true,
		"bibtex": "@article{usher_scalable_2019,\n
			journal = {Computer Graphics Forum},\n
			title = {{Scalable Ray Tracing Using the Distributed FrameBuffer}},\n
			author = {Usher, Will and Wald, Ingo and Amstutz, Jefferson and
				Günther, Johannes and Brownlee, Carson and Pascucci, Valerio},\n
			year = {2019},\n
			publisher = {The Eurographics Association and John Wiley & Sons Ltd.},\n
			ISSN = {1467-8659},\n
			DOI = {10.1111/cgf.13702}\n}",
		"abstract": "Image- and data-parallel rendering across multiple
			nodes on high-performance computing systems is widely used in visualization
			to provide higher frame rates, support large data sets, and
			render data in situ. Specifically for in situ visualization,
			reducing bottlenecks incurred by the visualization and compositing is
			of key concern to reduce the overall simulation runtime.
			Moreover, prior algorithms have been designed to support either
			image- or data-parallel rendering and impose restrictions on the
			data distribution, requiring different implementations for each
			configuration. In this paper, we introduce the Distributed FrameBuffer,
			an asynchronous image-processing framework for multi-node rendering.
			We demonstrate that our approach achieves performance superior to
			the state of the art for common use cases, while providing the
			flexibility to support a wide range of parallel rendering algorithms
			and data distributions. By building on this framework, we extend
			the open-source ray tracing library OSPRay with a data-distributed API, enabling
			its use in data-distributed and in situ visualization applications.",
		"teaser_caption": "Large-scale interactive visualization using the Distributed FrameBuffer.
			Top left: Image-parallel rendering of two transparent isosurfaces from
			the Richtmyer-Meshkov (516M triangles), 8FPS with a 2048<sup>2</sup>
			framebuffer using 16 Stampede2 Intel Xeon Platinum 8160 SKX nodes.
			Top right: Data-parallel rendering of the Cosmic Web (29B transparent spheres),
			2FPS at 2048<sup>2</sup> using 128 Theta Intel Xeon Phi Knight's Landing (KNL) nodes.
			Bottom: Data-parallel rendering of the 951GB DNS volume combined with a transparent
			isosurface (4.35B triangles), 5FPS at 4096x1024 using 64 Stampede2 Intel Xeon Phi
			KNL nodes.",
		"downloads": [
			{
				"title": "Code",
				"list": [
					{
						"title": "OSPRay",
						"link": "https://github.com/ospray/ospray",
						"icon": "fab fa-github"
					},
					{
						"title": "Distributed Rendering Benchmark App",
						"link": "https://github.com/Twinklebear/osp-distrib-viewer",
						"icon": "fab fa-github"
					},
					{
						"title": "IceT Comparison App",
						"link": "https://github.com/Twinklebear/osp-icet",
						"icon": "fab fa-github"
					}
				]
			}
		]
	},
	{
		"title": "Ray Tracing Generalized Tube Primitives: Method and Applications",
		"authors": "Mengjiao Han, Ingo Wald, Will Usher, Qi Wu, Feng Wang,
			Valerio Pascucci, Charles D. Hansen, and Chris R. Johnson",
		"venue": "Computer Graphics Forum (Proceedings of EuroVis)",
		"paper_pdf": "http://sci.utah.edu/~will/papers/tubes.pdf",
		"teaser": "https://i.imgur.com/tTv2l2j.jpg",
		"thumb": "https://i.imgur.com/gsgdwlZ.jpg",
		"year": 2019,
		"exact_date": "2019-07-10 00:00:00 -0600",
		"short_title": "tubes",
		"doi": "10.1111/cgf.13703",
		"bibtex": "@article{han_ray_2019,\n
			journal = {Computer Graphics Forum},\n
			title = {{Ray Tracing Generalized Tube Primitives: Method and Applications}},\n
			author = {Han, Mengjiao and Wald, Ingo and Usher, Will and
				Wu, Qi and Wang, Feng and Pascucci, Valerio and Hansen, Charles D.
				and Johnson, Chris R.},\n
			year = {2019},\n
			publisher = {The Eurographics Association and John Wiley & Sons Ltd.},\n
			ISSN = {1467-8659},\n
			DOI = {10.1111/cgf.13703}\n}",
		"abstract": "We present a general high-performance technique for ray tracing
			generalized tube primitives. Our technique efficiently supports
			tube primitives with fixed and varying radii, general acyclic graph
			structures with bifurcations,
			and correct transparency with interior surface removal.
			Such tube primitives are widely used in scientific visualization 
			to represent diffusion tensor imaging tractographies, neuron morphologies,
			and scalar or vector fields of 3D flow.
			We implement our approach within the OSPRay ray tracing framework,
			and evaluate it on a range of interactive visualization use cases
			of fixed- and varying-radius streamlines, pathlines, complex neuron morphologies,
			and brain tractographies. Our proposed approach provides interactive,
			high-quality rendering, with low memory overhead.",
		"teaser_caption": "Visualizations using our \"generalized tube\" primitives.
			(a): DTI tractography data, semi-transparent fixed-radius streamlines (218K line segments). 
			(b): A generated neuron assembly test case, streamlines with varying
			radii and bifurcations (3.2M l. s.).
			(c): Aneurysm morphology, semi-transparent streamlines with varying radii
			and bifurcations (3.9K l. s.)
			and an opaque center line with fixed radius and bifurcations (3.9K l. s.).
			(d): A tornado simulation, with radius used to encode the velocity magnitude (3.56M l. s.).
			(e): Flow past a torus, fixed-radius pathlines (6.5M l. s.).
			Rendered at: (a) 0.38FPS, (b) 7.2FPS, (c) 0.25FPS, (d) 18.8FPS, with a 2048x2048 framebuffer;
			(e) 23FPS with a 2048x786 framebuffer. Performance measured on a dual Intel Xeon
			E5-2640 v4 workstation, with shadows and ambient occlusion.",
		"supplemental_video": "https://www.youtube.com/embed/RB2yC5Io3JA",
		"downloads": [
			{
				"title": "Code",
				"list": [
					{
						"title": "OSPRay Module",
						"link": "https://github.com/MengjiaoH/ospray-module-tubes",
						"icon": "fab fa-github"
					}
				]
			}
		]
	},
	{
		"title": "Efficient Space Skipping and Adaptive Sampling of Unstructured
			Volumes Using Hardware Accelerated Ray Tracing",
		"authors": "Nate Morrical, Will Usher, Ingo Wald and Valerio Pascucci",
		"venue": "IEEE VIS Short Papers",
		"paper_pdf": "http://sci.utah.edu/~will/papers/vis19-space-skipping-short.pdf",
		"teaser": "https://i.imgur.com/NgoH3iw.jpg",
		"thumb": "https://i.imgur.com/deoKQwV.jpg",
		"presentation_video": "https://www.youtube.com/embed/205EtIX3H6k ",
		"year": 2019,
		"exact_date": "2019-10-20 00:00:00 -0600",
		"short_title": "rtx-space-skipping",
        "selected": true,
		"bibtex": "@inproceedings{morrical_skipping_2019,\n
			title = {Efficient {Space} {Skipping} and {Adaptive} {Sampling} of {Unstructured}
				{Volumes} {Using} {Hardware} {Accelerated} {Ray} {Tracing}},\n
			booktitle = {IEEE VIS 2019 - Short Papers},\n
			author = {Morrical, Nate, and Usher, Will and Wald, Ingo, and Pascucci, Valerio},\n
			year = {2019},\n}",
		"abstract": "Sample based ray marching is an effective method for direct volume
			rendering of unstructured meshes. However, sampling such meshes remains expensive,
			and strategies to reduce the number of samples taken have received relatively
			little attention. In this paper, we introduce a method for rendering unstructured 
			meshes using a combination of a coarse spatial acceleration structure and 
			hardware-accelerated ray tracing. Our approach enables efficient empty space
			skipping and adaptive sampling of unstructured meshes, and outperforms a reference
			ray marcher by up to 7×",
		"teaser_caption": "Performance improvement of our method on the 278 million
			tetrahedra Japan Earthquake data set. (a) A reference volume ray marcher
			without our method, at 0.9 FPS (1024<sup>2</sup> pixels) on an NVIDIA RTX 8000 GPU.
			(b) A heat map of relative cost per-pixel in (a).
			(c) and (d), the same, but now with our space skipping and adaptive sampling method,
			running at 7 FPS (7× faster)."
	},
	{
		"title": "RTX Beyond Ray Tracing: Exploring the Use of Hardware Ray Tracing
			Cores for Tet-Mesh Point Location",
		"authors": "Ingo Wald, Will Usher, Nate Morrical, Laura Lediaev, and Valerio Pascucci",
		"venue": "High Performance Graphics Short Papers",
		"paper_pdf": "http://sci.utah.edu/~will/papers/rtx-points-hpg19.pdf",
		"teaser": "https://i.imgur.com/clMvtsl.png",
		"thumb": "https://i.imgur.com/nbHnD45.jpg",
		"year": 2019,
		"exact_date": "2019-07-08 00:00:00 -0600",
		"short_title": "rtx-points",
		"doi": "10.2312/hpg.20191189",
        "selected": true,
		"bibtex": "@inproceedings{wald_rtx_2019,\n
			title = {{RTX} {Beyond} {Ray} {Tracing:} {Exploring} the {Use} of {Hardware}
				{Ray} {Tracing} {Cores} for {Tet}-{Mesh} {Point} {Location}},\n
			booktitle = {High-Performance Graphics - Short Papers},\n
			author = {Wald, Ingo and Usher, Will and Morrical, Nate
				and Lediaev, Laura and Pascucci, Valerio},\n
			year = {2019},\n
			DOI = {10.2312/hpg.20191189}\n}",
		"abstract": "We explore a first proof-of-concept example of creatively using the
			Turing generation's hardware ray tracing cores to solve a problem
			other than classical ray tracing, specifically, point location in
			unstructured tetrahedral meshes. Starting with a CUDA reference
			method, we describe and evaluate three different approaches to
			reformulate this problem in a manner that allows it to be mapped
			to these new hardware units. Each variant replaces the simpler problem
			of point queries with the more complex one of ray queries; however,
			thanks to hardware acceleration, these approaches are actually
			faster than the reference method.",
		"teaser_caption": "a-c) Illustrations of the tetrahedral mesh point location
		kernels evaluated in this paper. a) Our reference method builds a BVH over
		the tets and performs both BVH traversal and point-in-tet tests in software
		(black) using CUDA. b) <font face='monospace'>rtx-bvh</font> uses an
		RTX-accelerated BVH over tets and
		triggers hardware BVH traversal (green) by tracing infinitesimal rays at the
		sample points, while still performing point-tet tests in software (black).
		c) <font face='monospace'>rtx-rep-faces</font> and
		<font face='monospace'>rtx-shrd-faces</font> use both hardware BVH traversal and
		triangle intersection (green) by tracing rays against the tetrahedras' faces.
		d) An image from the unstructured-data volume ray marcher used to evaluate
		our point location kernels, showing the 35.7M tet Agulhas Current data
		set rendered interactively on an NVIDIA TITAN RTX (34FPS at 1024<sup>2</sup> pixels)"
	},
	{
		"title": "Spatially-aware Parallel I/O for Particle Data",
		"authors": "Sidharth Kumar, Steve Petruzza, Will Usher, and Valerio Pascucci",
		"venue": "48th International Conference on Parallel Processing (ICPP)",
		"paper_pdf": "http://sci.utah.edu/~will/papers/icpp19.pdf",
		"teaser": "/assets/img/icpp-two-phase-io.svg",
		"thumb": "https://i.imgur.com/GWCom0N.jpg",
		"year": 2019,
		"exact_date": "2019-08-05 00:00:00 -0600",
		"short_title": "icpp19",
		"doi": "10.1145/3337821.3337875",
        "selected": true,
		"bibtex": "@inproceedings{kumar_spatially-aware_2019,\n
			title = {{Spatially}-aware {Parallel} {I}/{O} for {Particle} {Data}},\n
			booktitle = {Proceedings of the 48th International Conference on Parallel Processing},\n
			series = {ICPP 2019},
			author = {Kumar, Sidharth and Petruzza, Steve and Usher, Will and Pascucci, Valerio},\n
			year = {2019},\n
			doi = {10.1145/3337821.3337875}\n}",
		"abstract": "Particle data are used across a diverse set of large scale simulations,
			for example, in cosmology, molecular dynamics and combustion. 
			At scale these applications generate tremendous amounts of data, which is
			often saved in an unstructured format that does not preserve spatial locality;
			resulting in poor read performance for post-processing analysis and visualization tasks,
			which typically make spatial queries.
			In this work, we explore some of the challenges of large scale particle data 
			management, and introduce new techniques to perform scalable, spatially-aware write and read operations.
			We propose an adaptive aggregation technique to improve the performance of data aggregation, for
			both uniform and non-uniform particle distributions. Furthermore, we enable efficient read operations
			by employing a level of detail re-ordering and a multi-resolution layout. Finally, we demonstrate the
			scalability of our techniques with experiments
			on large scale simulation workloads up to 256K cores on two different leadership supercomputers, Mira and Theta.",
		"teaser_caption": "An illustration of our two-phase I/O approach, which takes spatial locality into consideration."
	},
	{
		"title": "CPU Isosurface Ray Tracing of Adaptive Mesh Refinement Data",
		"authors": "Feng Wang, Ingo Wald, Qi Wu, Will Usher and Chris R. Johnson",
		"venue": "IEEE Transactions on Visualization and Computer Graphics",
		"paper_pdf": "http://sci.utah.edu/~will/papers/amr-isosurface.pdf",
		"teaser": "https://i.imgur.com/PqmRTuz.jpg",
		"thumb": "https://i.imgur.com/Yzxf5OT.jpg",
		"year": 2019,
		"exact_date": "2019-01-01 00:00:00 -0600",
		"short_title": "amr-iso",
		"doi": "10.1109/TVCG.2018.2864850",
        "selected": true,
		"bibtex": "@article{Wang_AMR_Iso_2019,\n
			author={F. Wang and I. Wald and Q. Wu and W. Usher and C. R. Johnson},\n
			journal={{IEEE} {Transactions} on {Visualization} and {Computer} {Graphics}},\n
			title={{CPU} {Isosurface} {Ray} {Tracing} of {Adaptive} {Mesh} {Refinement} {Data}},\n
			year={2019},\n
			doi={10.1109/TVCG.2018.2864850},\n}",
		"abstract": "Adaptive mesh refinement (AMR) is a key technology for large-scale simulations that allows for adaptively changing the simulation mesh resolution, resulting in significant computational and storage savings. However, visualizing such AMR data poses a significant challenge due to the difficulties introduced by the hierarchical representation when reconstructing continuous field values. In this paper, we detail a comprehensive solution for interactive isosurface rendering of block-structured AMR data. We contribute a novel reconstruction strategy&mdash;the <i>octant</i> method&mdash;which is continuous, adaptive and simple to implement. Furthermore, we present a generally applicable hybrid implicit isosurface ray-tracing method, which provides better rendering quality and performance than the built-in sampling-based approach in OSPRay. Finally, we integrate our <i>octant</i> method and hybrid isosurface geometry into OSPRay as a module, providing the ability to create high-quality interactive visualizations combining volume and isosurface representations of BS-AMR data. We evaluate the rendering performance, memory consumption and quality of our method on two gigascale block-structured AMR datasets.",
		"teaser_caption": "High-fidelity isosurface visualizations of gigascale block-structured adaptive mesh refinement (BS-AMR) data using our method. Left: a 28GB GR-Chombo simulation of gravitational waves resulting from the collision of two black holes. Middle and Right: a 57GB AMR dataset computed with LAVA at NASA, simulating multiple fields over the landing gear of an aircraft. Middle: isosurface representation of the vorticity, rendered with path tracing. Right: a combined visualization of volume rending and an isosurface of the pressure over the landing gear, rendered with OSPRay's SciVis renderer.  Using our approach for ray tracing such AMR data, we can interactively render crack-free implicit isosurfaces in combination with direct volume rendering and advanced shading effects like transparency, ambient occlusion and path tracing.",
		"downloads": [
			{
				"title": "Code",
				"list": [
					{
						"title": "AMR Isosurface module for OSPRay",
						"link": "https://github.com/ethan0911/module-impi",
						"icon": "fab fa-github"
					}
				]
			}
		]
	},
	{
		"title": "A Virtual Reality Visualization Tool for Neuron Tracing",
		"authors": "Will Usher, Pavol Klacansky, Frederick Federer, Peer-Timo Bremer,
				Aaron Knoll, Jeff Yarch, Alessandra Angelucci, and Valerio Pascucci",
		"venue": "IEEE Transactions on Visualization and Computer Graphics",
		"paper_pdf": "http://sci.utah.edu/~will/papers/vrnt/vr-neuron-tracing.pdf",
		"teaser": "https://i.imgur.com/2rmTeh2.png",
		"thumb": "https://i.imgur.com/2iL7UdK.png",
		"year": 2018,
		"exact_date": "2018-01-01 00:00:00 -0600",
		"short_title": "vrnt",
		"doi": "10.1109/TVCG.2017.2744079",
		"supplemental_video": "https://www.youtube.com/embed/lTNef_kbLKg",
		"presentation_video": "https://www.youtube.com/embed/XzmYdl9rFs0",
        "selected": true,
		"downloads": [
			{
				"title": "Videos",
				"list": [
					{
						"title": "Analyzing an Expert Session",
						"link": "https://youtu.be/L5tsU8TtgkU"
					},
					{
						"title": "ZED Mixed Reality Tracing Video",
						"link": "https://youtu.be/qIPW3ut4sv8"
					},
					{
						"title": "ZED Mixed Reality Prototype",
						"link": "https://youtu.be/t6rPU6hy5tk"
					},
					{
						"title": "VIS17 Fast Forward",
						"link": "https://youtu.be/mZ6YT_y7Kx0"
					}
				]
			},
			{
				"title": "Software",
				"list": [
					{
						"title": "Available on Steam!",
						"link": "https://store.steampowered.com/app/791040/Virtual_Reality_Neuron_Tracer/",
						"icon": "fab fa-steam"
					}
				]
			}
		],
		"bibtex": "@article{Usher_VRNT_2018,\n
			author={W. Usher and P. Klacansky and F. Federer and P. T. Bremer and A. Knoll and J. Yarch and A. Angelucci and V. Pascucci},\n
			journal={{IEEE} {Transactions} on {Visualization} and {Computer} {Graphics}},\n
			title={A {Virtual} {Reality} {Visualization} {Tool} for {Neuron} {Tracing}},\n
			year={2018},\n
			volume={24},\n
			number={1},\n
			pages={994-1003},\n
			doi={10.1109/TVCG.2017.2744079},\n
			ISSN={1077-2626},\n
			month={Jan},\n}",
		"abstract": "Tracing neurons in large-scale microscopy data is crucial to establishing a wiring diagram of the brain, which is needed to understand how neural circuits in the brain process information and generate behavior. Automatic techniques often fail for large and complex datasets, and connectomics researchers may spend weeks or months manually tracing neurons using 2D image stacks. We present a design study of a new virtual reality (VR) system, developed in collaboration with trained neuroanatomists, to trace neurons in microscope scans of the visual cortex of primates. We hypothesize that using consumer-grade VR technology to interact with neurons directly in 3D will help neuroscientists better resolve complex cases and enable them to trace neurons faster and with less physical and mental strain. We discuss both the design process and technical challenges in developing an interactive system to navigate and manipulate terabyte-sized image volumes in VR. Using a number of different datasets, we demonstrate that, compared to widely used commercial software, consumer-grade VR presents a promising alternative for scientists.",
		"teaser_caption": "A screenshot of our VR neuron tracing tool using the isosurface rendering mode. The dark gray floor represents the extent of the tracked space. Users can orient themselves in the dataset via the minimap (right), which shows the world extent in blue, the current focus region in orange, and the previously traced neuronal structures. The focus region is displayed in the center of the space. The 3D interaction and visualization provides an intuitive environment for exploring the data and a natural interface for neuron tracing, resulting in faster, high-quality traces with less fatigue reported by users compared to existing 2D tools."
	},
	{
		"title": "libIS: A Lightweight Library for Flexible In Transit Visualization",
		"authors": "Will Usher, Silvio Rizzi, Ingo Wald, Jefferson Amstutz,
			Joseph Insley, Venkatram Vishwanath, Nicola Ferrier,
			Michael E. Papka, and Valerio Pascucci",
		"venue": "ISAV: In Situ Infrastructures for Enabling Extreme-Scale
			Analysis and Visualization (ISAV '18)",
		"paper_pdf": "http://sci.utah.edu/~will/papers/libis-isav18.pdf",
		"teaser": "https://i.imgur.com/UYlTqhT.png",
		"thumb": "https://i.imgur.com/aCX8XH2.png",
		"year": 2018,
		"exact_date": "2018-11-12 00:00:00 -0600",
		"short_title": "libis-isav18",
		"doi": "10.1145/3281464.3281466",
		"supplemental_video": "https://www.youtube.com/embed/YUH55CvPmxg",
        "selected": true,
		"bibtex": "@inproceedings{usher_libis_2018,\n
			title = {{libIS}: {A} {Lightweight} {Library} for
				{Flexible} {In} {Transit} {Visualization}},\n
			year = {2018},\n
			booktitle = {ISAV: In Situ Infrastructures for Enabling
				Extreme-Scale Analysis and Visualization},\n
			series = {ISAV'18},\n
			author = {Usher, Will and Rizzi, Silvio and Wald, Ingo
				and Amstutz, Jefferson and Insley, Joseph and
				Vishwanath, Venkatram and Ferrier, Nicola and
				Papka, Michael E. and Pascucci, Valerio},\n
			doi={10.1145/3281464.3281466}\n
		}",
		"teaser_caption": "Interactive in situ visualization of a 172k atom simulation of silicene formation with 128 LAMMPS ranks sending to 16 OSPRay renderer ranks, all executed on Theta in the mpi-multi configuration. When taking four ambient occlusion samples per-pixel, our viewer averages 7FPS at 1024x1024. Simulation dataset is courtesy of <a href='https://pubs.rsc.org/en/content/articlelanding/2017/nr/c7nr03153j#!divAbstract'>Cherukara et al.</a>",
		"abstract": "As simulations grow in scale, the need for in situ analysis methods to handle the large data produced grows correspondingly. One desirable approach to in situ visualization is in transit visualization. By decoupling the simulation and visualization code, in transit approaches alleviate common difficulties with regard to the scalability of the analysis, ease of integration, usability, and impact on the simulation. We present libIS, a lightweight, flexible library which lowers the bar for using in transit visualization. Our library works on the concept of abstract regions of space containing data, which are transferred from the simulation to the visualization clients upon request, using a client-server model. We also provide a SENSEI analysis adaptor, which allows for transparent deployment of in transit visualization. We demonstrate the flexibility of our approach on batch analysis and interactive visualization use cases on different HPC resources.",
		"downloads": [
			{
				"title": "Code",
				"list": [
					{
						"title": "libIS",
						"link": "https://github.com/Twinklebear/libIS",
						"icon": "fab fa-github"
					},
					{
						"title": "SENSEI Adaptor for libIS and LAMMPS",
						"link": "https://gitlab.kitware.com/sensei/sensei/tree/lammps/miniapps/lammps",
						"icon": "fab fa-gitlab"
					},
					{
						"title": "Theta SSH Tunneling Script",
						"link": "https://github.com/Twinklebear/theta-tunnel",
						"icon": "fab fa-github"
					}
				]
			}
		]
	},
	{
		"title": "VisIt-OSPRay: Toward an Exascale Volume Visualization System",
		"authors": "Qi Wu, Will Usher, Steve Petruzza, Sidharth Kumar, Feng Wang,
			Ingo Wald, Valerio Pascucci, and Charles D. Hansen",
		"venue": "Eurographics Symposium on Parallel Graphics and Visualization",
		"paper_pdf": "http://sci.utah.edu/~will/papers/visit-ospray-egpgv18.pdf",
		"teaser": "https://i.imgur.com/JgAOmst.png",
		"thumb": "https://i.imgur.com/RxCFTxo.jpg",
		"year": 2018,
		"exact_date": "2018-06-04 00:00:00 -0600",
		"short_title": "visit-ospray",
		"doi": "10.2312/pgv.20181091",
        "selected": true,
		"bibtex": "@inproceedings{Wu_VisItOSPRay_2018,\n
			booktitle = {Eurographics Symposium on Parallel Graphics and Visualization},\n
			editor = {Hank Childs and Fernando Cucchietti},\n
			title = {{VisIt}-{OSPRay}: {Toward} an {Exascale} {Volume} {Visualization} {System}},\n
			author = {Wu, Qi and Usher, Will and Petruzza, Steve and Kumar, Sidharth and Wang, Feng and Wald, Ingo and Pascucci, Valerio and Hansen, Charles D.},\n
			year = {2018},\n
			publisher = {The Eurographics Association},\n
			ISSN = {1727-348X},\n
			ISBN = {978-3-03868-054-3},\n
			DOI = {10.2312/pgv.20181091}\n
		}",
		"abstract": "Large-scale simulations can easily produce data in excess of what can be efficiently visualized using production visualization software, making it challenging for scientists to gain insights from the results of these simulations. This trend is expected to grow with exascale. To meet this challenge, and run on the highly parallel hardware being deployed on HPC system, rendering systems in production visualization software must be redesigned to perform well at these new scales and levels of parallelism. In this work, we present VisIt-OSPRay, a high-performance, scalable, hybrid-parallel rendering system in VisIt, using OSPRay and IceT, coupled with PIDX for scalable I/O. We examine the scalability and memory efficiency of this system and investigate further areas for improvement to prepare VisIt for upcoming exascale workloads.",
		"teaser_caption": "High-quality interactive volume visualization using VisIt-OSPRay: <b>a)</b> volume rendering of O<sub>2</sub> concentration inside a combustion chamber, data courtesy of the <a href=\"http://ccmsc.sci.utah.edu/\">University of Utah CCMSC</a>; <b>b)</b> volume rendering of the Richtmyer-Meshkov Instability; <b>c)</b> visualization of a supernova simulation; <b>d)</b> visualization of the aneurysm dataset using volume rendering and streamlines; <b>e)</b> scalable volume rendering of the 966GB DNS data on 64 Stampede2 Intel Xeon Phi Knight's Landing nodes."
	},
	{
		"title": "Scalable Data Management of the Uintah Simulation Framework for Next-Generation Engineering Problems with Radiation",
		"authors": "Sidharth Kumar, Alan Humphrey, Will Usher, Steve Petruzza,
				Brad Peterson, John A. Schmidt, Derek Harris, Ben Isaac,
				Jeremy Thornock, Todd Harman, Valerio Pascucci, and Martin Berzins",
		"venue": "Supercomputing Frontiers",
		"paper_pdf": "http://sci.utah.edu/~will/papers/scasia18.pdf",
		"thumb": "https://i.imgur.com/w5YWvd6.png",
		"year": 2018,
		"exact_date": "2018-03-20 00:00:00 -0600",
		"short_title": "scasia18",
		"doi": "10.1007/978-3-319-69953-0_13",
		"downloads": [
			{
				"title": "Videos",
				"list": [
					{
						"title": "Uintah UASC Coal Boiler Visualization",
						"link": "https://youtu.be/vpJtHTzArq4"
					}
				]
			}
		],
		"bibtex": "@incollection{kumar_scalable_2018,\n
			title = {Scalable {Data} {Management} of the {Uintah} {Simulation} {Framework} for {Next}-{Generation} {Engineering} {Problems} with {Radiation}},\n
			volume = {10776},\n
			isbn = {978-3-319-69952-3 978-3-319-69953-0},\n
			booktitle = {Supercomputing {Frontiers}},\n
			publisher = {Springer International Publishing},\n
			author = {Kumar, Sidharth and Humphrey, Alan and Usher, Will and Petruzza, Steve and Peterson, Brad and Schmidt, John A. and Harris, Derek and Isaac, Ben and Thornock, Jeremy and Harman, Todd and Pascucci, Valerio and Berzins, Martin},\n
			editor = {Yokota, Rio and Wu, Weigang},\n
			year = {2018},\n
			doi = {10.1007/978-3-319-69953-0_13},\n
			pages = {219--240},\n}",
			"abstract": "The need to scale next-generation industrial engineering
				problems to the largest computational platforms presents unique challenges.
				This paper focuses on data management related problems faced
				by the Uintah simulation framework at a production scale of 260K processes.
				Uintah provides a highly scalable asynchronous many-task runtime
				system, which in this work is used for the modeling of a 1000
				megawatt electric (MWe) ultra-supercritical (USC) coal boiler. At 260K
				processes, we faced both parallel I/O and visualization related challenges,
				e.g., the default file-per-process I/O approach of Uintah did not scale
				on Mira. In this paper we present a simple to implement, restructuring
				based parallel I/O technique. We impose a restructuring step that
				alters the distribution of data among processes. The goal is to distribute
				the dataset such that each process holds a larger chunk of data, which is
				then written to a file independently. This approach finds a middle ground
				between two of the most common parallel I/O schemes–file per process
				I/O and shared file I/O–in terms of both the total number of generated
				files, and the extent of communication involved during the data aggregation
				phase. To address scalability issues when visualizing the simulation
				data, we developed a lightweight renderer using OSPRay, which allows
				scientists to visualize the data interactively at high quality and make
				production movies. Finally, this work presents a highly efficient and scalable
				radiation model based on the sweeping method, which significantly
				outperforms previous approaches in Uintah, like discrete ordinates. The
				integrated approach allowed the USC boiler problem to run on 260K
				CPU cores on Mira."
	},
	{
		"title": "CPU Volume Rendering of Adaptive Mesh Refinement Data",
		"authors": "Ingo Wald, Carson Brownlee, Will Usher, and Aaron Knoll",
		"venue": "SIGGRAPH Asia 2017 Symposium on Visualization",
		"paper_pdf": "http://sci.utah.edu/~will/papers/cvamr/cvamr.pdf",
		"teaser": "https://i.imgur.com/CqZc3VJ.png",
		"thumb": "https://i.imgur.com/JFShB4G.png",
		"year": 2017,
		"exact_date": "2017-11-27 00:00:00 -0600",
		"short_title": "cvamr",
		"doi": "10.1145/3139295.3139305",
        "selected": true,
		"bibtex": "@inproceedings{Wald_CVAMR_2017,\n
			author = {Wald, Ingo and Brownlee, Carson and Usher, Will and Knoll, Aaron},\n
			title = {CPU Volume Rendering of Adaptive Mesh Refinement Data},\n
			booktitle = {SIGGRAPH Asia 2017 Symposium on Visualization},\n
			series = {SA '17},\n
			year = {2017},\n
			isbn = {978-1-4503-5411-0},\n
			location = {Bangkok, Thailand},\n
			pages = {9:1--9:8},\n
			articleno = {9},\n
			numpages = {8},\n
			url = {http://doi.acm.org/10.1145/3139295.3139305},\n
			doi = {10.1145/3139295.3139305},\n
			acmid = {3139305},\n
			publisher = {ACM},\n
			address = {New York, NY, USA},\n
		}",
		"abstract": "Adaptive Mesh Refinement (AMR) methods are widespread
		in scientific computing, and visualizing the resulting data with
		efficient and accurate rendering methods can be vital for enabling 
		interactive data exploration.
		In this work, we
		detail a comprehensive solution for directly volume rendering block-structured 
		(Berger-Colella) AMR data in the
		OSPRay interactive CPU ray tracing framework. In particular, we
		contribute a general method for representing and traversing AMR data
		using a kd-tree structure, and four different reconstruction
		options, one of which in particular (the basis function approach)
		is novel compared to existing methods. We demonstrate our system on two
		types of block-structured AMR data and compressed scalar
		field data, and show how it can be easily used in existing production-ready
		applications through a prototypical integration in the widely used visualization program ParaView.",
		"teaser_caption": "Two examples of our method (integrated within the OSPRay ray tracer):
		Left: 1.8GB Cosmos AMR data, rendered in ParaView. Right: a 57GB NASA Chombo simulation,
		rendered with ambient occlusion and shadows alongside mesh geometry."
	},
	{
		"title": "Progressive CPU Volume Rendering with Sample Accumulation",
		"authors": "Will Usher, Jefferson Amstutz, Carson Brownlee, Aaron Knoll, and Ingo Wald",
		"venue": "Eurographics Symposium on Parallel Graphics and Visualization",
		"paper_pdf": "http://sci.utah.edu/~will/papers/savr/savr.pdf",
		"teaser": "https://i.imgur.com/15y1f8I.png",
		"thumb": "https://i.imgur.com/tdxjYs3.png",
		"short_title": "savr",
		"doi": "10.2312/pgv.20171090",
		"year": 2017,
		"exact_date": "2017-06-12 00:00:00 -0600",
		"bibtex": "@inproceedings{Usher_SAVR_2017,\n
			booktitle={Eurographics Symposium on Parallel Graphics and Visualization},\n
			editor={Alexandru Telea and Janine Bennett},\n
			title={{Progressive CPU Volume Rendering with Sample Accumulation}},\n
			author={Usher, Will and Amstutz, Jefferson and Brownlee, Carson and Knoll, Aaron and Wald, Ingo},\n
			year={2017},\n
			publisher={The Eurographics Association},\n
			issn={1727-348X},\n
			isbn={978-3-03868-034-5},\n
			doi={10.2312/pgv.20171090},\n}",
		"abstract": "We present a new method for progressive volume rendering by accumulating object-space samples over successively rendered frames. Existing methods for progressive refinement either use image space methods or average pixels over frames, which can blur features or integrate incorrectly with respect to depth. Our approach stores samples along each ray, accumulates new samples each frame into a buffer, and progressively interleaves and integrates these samples. Though this process requires additional memory, it ensures interactivity and is well suited for CPU architectures with large memory and cache. This approach also extends well to distributed rendering in cluster environments. We implement this technique in Intel’s open source OSPRay CPU ray tracing framework and demonstrate that it is particularly useful for rendering volumetric data with costly sampling functions.",
		"teaser_caption": "(a-c) Progressive refinement with Sample-Accumulation Volume Rendering (SAVR) on the 40GB Landing Gear AMR dataset using a prototype AMR sampler. The SAVR algorithm correctly accumulates frames to progressively refine the image. After 16 frames of accumulation the volume is sampled at the Nyquist limit, with some small noise, by 32 frames the noise has been removed. SAVR extends to distributed data, in (d) we show the 1TB DNS dataset, a 10240×7680×1536 uniform grid, rendered interactively across 64 second-generation Intel Xeon Phi \"Knights Landing\" (KNL) processor nodes on Stampede 1.5 at a 6144×1024 resolution. While interacting, our method achieves around 5.73 FPS.",
		"downloads": [
			{
				"title": "Code",
				"list": [
					{
						"title": "SAVR module for OSPRay",
						"link": "https://github.com/ospray/module_savr",
						"icon": "fab fa-github"
					}
				]
			}
		]
	},
	{
		"title": "In Situ Exploration of Particle Simulations with CPU Ray Tracing",
		"authors": "Will Usher, Ingo Wald, Aaron Knoll, Michael E. Papka, and Valerio Pascucci",
		"venue": "Supercomputing Frontiers and Innovations",
		"paper_pdf": "http://sci.utah.edu/~will/papers/in_situ_particles/in_situ_particles.pdf",
		"teaser": "https://i.imgur.com/DO3JqOb.png",
		"thumb": "https://i.imgur.com/gieTAy3.png",
		"short_title": "isp-jsfi",
		"doi": "10.14529/jsfi160401",
		"year": 2016,
		"exact_date": "2016-10-01 00:00:00 -0600",
		"bibtex": "@article{Usher_InSituParticles_2016,\n
			author={Will Usher and Ingo Wald and Aaron Knoll and Michael E. Papka and Valerio Pascucci},\n
			title={In {Situ} {Exploration} of {Particle} {Simulations} with {CPU} {Ray} {Tracing}},\n
			journal={{Supercomputing} {Frontiers} and {Innovations}},\n
			volume={3},\n
			number={4},\n
			year={2016},\n
			issn={2313-8734},\n
			doi={10.14529/jsfi160401},\n}",
		"abstract": "We present a system for interactive in situ visualization of large particle simulations, suitable for general CPU-based HPC architectures. As simulations grow in scale, in situ methods are needed to alleviate IO bottlenecks and visualize data at full spatio-temporal resolution. We use a lightweight loosely-coupled layer serving distributed data from the simulation to a data-parallel renderer running in separate processes. Leveraging the OSPRay ray tracing framework for visualization and balanced P-k-d trees, we can render simulation data in real-time, as they arrive, with negligible memory overhead. This flexible solution allows users to perform exploratory in situ visualization on the same computational resources as the simulation code, on dedicated visualization clusters or remote workstations, via a standalone rendering client that can be connected or disconnected as needed. We evaluate this system on simulations with up to 227M particles in the LAMMPS and Uintah computational frameworks, and show that our approach provides many of the advantages of tightly-coupled systems, with the flexibility to render on a wide variety of remote and co-processing resources.",
		"teaser_caption": "A coal particle combustion simulation in Uintah at three different timesteps with (left to right): 34.61M, 48.46M and 55.39M particles, with attribute based culling showing the full jet (top) and the front in detail (bottom). Using our in situ library to query and send data to our rendering client in OSPRay these images are rendered interactively with ambient occlusion, averaging around 13 FPS at 1920×1080. The renderer is run on 12 nodes of the Stampede supercomputer and pulls data from a Uintah simulation running on 64 processes (4 nodes). Our loosely-coupled in situ approach allows for live exploration at the full temporal fidelity of the simulation, without prohibitive IO cost.",
		"downloads": [
			{
				"title": "Code",
				"list": [
					{
						"title": "In Situ Particles module for OSPRay",
						"link": "https://github.com/Twinklebear/in-situ-particles",
						"icon": "fab fa-github"
					}
				]
			}
		]
	},
	{
		"title": "VTK-m: Accelerating the Visualization Toolkit for Massively Threaded Architectures",
		"authors": "Kenneth Moreland, Christopher Sewell, William Usher, Li-ta Lo, Jeremy Meredith,
				David Pugmire, James Kress, Hendrik Schroots, Kwan-Liu Ma, Hank Childs, Matthew Larsen,
				Chun-Ming Chen, Robert Maynard, and Berk Geveci",
		"venue": "IEEE Computer Graphics and Applications",
		"paper_pdf": "http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7466740",
		"thumb": "https://i.imgur.com/VEcRSDW.jpg",
		"doi": "10.1109/MCG.2016.48",
		"year": 2016,
		"exact_date": "2016-05-09 00:00:00 -0600",
		"short_title": "vtkm",
		"bibtex": "@article{Moreland_VTKm_2016,\n
			author={K. Moreland and C. Sewell and W. Usher and L. t. Lo and J. Meredith and D. Pugmire and J. Kress and H. Schroots and K. L. Ma and H. Childs and M. Larsen and C. M. Chen and R. Maynard and B. Geveci},\n
			journal={{IEEE} {Computer} {Graphics} and {Applications}},\n
			title={{VTK-m}: {Accelerating} the {Visualization} {Toolkit} for {Massively} {Threaded} {Architectures}},\n
			year={2016},\n}",
		"abstract": "One of the most critical challenges for high-performance computing (HPC) scientific visualization is execution on massively threaded processors. Of the many fundamental changes we are seeing in HPC systems, one of the most profound is a reliance on new processor types optimized for execution bandwidth over latency hiding. Our current production scientific visualization software is not designed for these new types of architectures. To address this issue, the VTK-m framework serves as a container for algorithms, provides flexible data representation, and simplifies the design of visualization algorithms on new and future computer architecture.",
		"downloads": [
			{
				"title": "Code",
				"list": [
					{
						"title": "VTK-m",
						"link": "https://gitlab.kitware.com/vtk/vtk-m",
						"icon": "fab fa-gitlab"
					}
				]
			}
		]
	},
	{
		"title": "CPU Ray Tracing Large Particle Data with Balanced P-k-d Trees",
		"authors": "Ingo Wald, Aaron Knoll, Gregory P. Johnson, Will Usher, Valerio Pasucci, and Michael E. Papka",
		"venue": "IEEE Vis (conference)",
		"paper_pdf": "http://sci.utah.edu/~will/papers/pkd/pkd_tree.pdf",
		"doi": "10.1109/SciVis.2015.7429492",
		"teaser": "https://i.imgur.com/1YNpRJ1.png",
		"thumb": "https://i.imgur.com/qpTN5kR.png",
		"short_title": "pkd",
		"year": 2015,
		"exact_date": "2015-10-25 00:00:00 -0600",
        "selected": true,
		"bibtex": "@inproceedings{Wald_PKD_2015,\n
			author={I. Wald and A. Knoll and G. P. Johnson and W. Usher and V. Pascucci and M. E. Papka},\n
			booktitle={2015 IEEE Scientific Visualization Conference (SciVis)},\n
			title={{CPU} ray tracing large particle data with balanced {P-k-d} trees},\n
			year={2015},\n
			doi={10.1109/SciVis.2015.7429492},\n}",
		"abstract": "We present a novel approach to rendering large particle data sets from molecular dynamics, astrophysics and other sources. We employ a new data structure adapted from the original balanced k-d tree, which allows for representation of data with trivial or no overhead. In the OSPRay visualization framework, we have developed an efficient CPU algorithm for traversing, classifying and ray tracing these data. Our approach is able to render up to billions of particles on a typical workstation, purely on the CPU, without any approximations or level-of-detail techniques, and optionally with attribute-based color mapping, dynamic range query, and advanced lighting models such as ambient occlusion and path tracing.",
		"teaser_caption": "Full-detail ray tracing of giga-particle data sets. From left to right: CosmicWeb early universe data set from a P3D simulation with 29 billion particles; a 100 million atom molecular dynamics Al<sub>2</sub>O<sub>3</sub>−SiC materials fracture simulation; and a 1.3 billion particle Uintah MPM detonation simulation. Using a quad-socket, 72-core 2.5GHz Intel Xeon E7-8890 v3 Processor with 3TB RAM and path-tracing with progressive refinement at 1 sample per pixel, these far and close images (above and below) are rendered at 1.6 (far) / 1.0 (close) FPS (left), 2.0 / 1.2 FPS (center), and 1.0 / 0.9 FPS (right), respectively, at 4K (3840×2160) resolution. All examples use our balanced P-k-d tree, an acceleration structure which requires little or no memory cost beyond the original data.",
		"downloads": [
			{
				"title": "Code",
				"list": [
					{
						"title": "PKD module for OSPRay",
						"link": "https://github.com/ingowald/ospray-module-pkd",
						"icon": "fab fa-github"
					}
				]
			}
		]
	}
]

