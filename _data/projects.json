[

	{
		"title": "tray_rust",
		"layout": 6,
		"gh_user": "Twinklebear",
		"gh_repo": "tray_rust",
		"thumb": "https://i.imgur.com/SHCFQ5C.png",
		"wide_img": "https://www.youtube.com/embed/5XY8ua4ysvM",
		"img": [
			"https://i.imgur.com/ywNdfGD.png",
			"https://i.imgur.com/yiH8xsI.png",
			"https://i.imgur.com/iOU7Xys.png",
			"https://i.imgur.com/Dzg3Fxr.png"
		],
		"description": "tray_rust is a toy physically based ray tracer built off of the techniques
			discussed in [Physically Based Rendering](http://pbrt.org/). It began life as a port of
			[tray](https://github.com/Twinklebear/tray) to [Rust](http://www.rust-lang.org)
			to check out the language.
			The renderer is currently capable of path tracing, supports triangle meshes (MTL support coming soon),
			and various physically based material models (including measured data from the
			[MERL BRDF Database](http://www.merl.com/brdf/)) along with rigid body animation
			and image-parallel distributed rendering. More
			details on rendering performance and other features can be found in the readme
			[README](https://github.com/Twinklebear/tray_rust/blob/master/README.md).\n\n

			tray_rust can also render animations, and is the renderer I've used for the University of Utah's
			[teapot rendering competition](http://graphics.cs.utah.edu/trc/) after the first year.
			The video is my submission from 2016. To simplify making scenes
			for tray_rust I also wrote a [Blender plugin](https://github.com/Twinklebear/tray_rust_blender)
			which I used to make the animation
			shown, and is the easiest way to create scenes for the renderer though some features
			are still needed."
	},
	{
		"title": "ispc-rs",
		"layout": 1,
		"gh_user": "Twinklebear",
		"gh_repo": "ispc-rs",
		"thumb": "https://i.imgur.com/sbZCnq6.png",
		"img": "http://i.imgur.com/ACzYqAm.png",
		"description": "ispc-rs is a small Rust library meant to be used as a compile time dependency
			for Rust projects to allow them to build and link with code written in
			[ISPC](https://ispc.github.io/). ISPC is a language that makes it possible to take advantage
			the CPU's vector units without needing hand-written intrinsics. Through this library it's easy
			to write very fast vector code in ISPC and link it with (still quick!) higher-level code
			in Rust to get a good balance of high performance and ease of use.\n\n
			The images shown above are from the
			[rt example](https://github.com/Twinklebear/ispc-rs/tree/master/examples/rt) (top row)
			which demonstrates a simple fast and parallel path tracer
			and the [ddvol example](https://github.com/Twinklebear/ispc-rs/tree/master/examples/ddvol)
			(bottom row) which is a scientific visualization volume
			renderer. Both use higher level Rust code to read in a scene file and setup
			the objects in the scene and then call into ISPC to render in parallel, making
			good use of the CPU's SIMD units."
	},
	{
		"title": "ChameleonRT",
		"layout": 2,
		"gh_user": "Twinklebear",
		"gh_repo": "ChameleonRT",
		"thumb": "https://i.imgur.com/EGZPkkV.jpg",
		"img": [
            "https://i.imgur.com/KA2dqKn.jpg",
            "https://i.imgur.com/MJ8FRZ1.jpg",
            "https://i.imgur.com/pV4OFlL.jpg"
        ],
		"description": "ChameleonRT is an example interactive path tracer which runs on multiple
        CPU and GPU ray tracing backends: Embree, DXR, OptiX, and Vulkan. Each backend uses an
        identical path tracer which supports the Disney BSDF material model to provide the same
        rendered result in each."
	},
	{
		"title": "topo-vol",
		"layout": 4,
		"gh_user": "Twinklebear",
		"gh_repo": "topo-vol",
		"thumb": "https://i.imgur.com/Lb1UAZM.png",
		"img": "http://i.imgur.com/0geW8ma.png",
		"description": "topo-vol is a topology guided volume exploration and analysis tool, written for
			the final project in [Bei Wang's Computational Topology Course](http://www.sci.utah.edu/~beiwang/teaching/cs6170-spring-2017/schedule.html).
			It is built on top of the [Topology ToolKit](https://topology-tool-kit.github.io/) and
			[VTK](http://www.vtk.org/) for computation, and uses [ImGui](https://github.com/ocornut/imgui)
			and a custom rendering system for the UI and volume rendering. By computing relevant topological
			structures (e.g. the contour tree) and classifying segments of data corresponding to the
			branches in this tree we can avoid occlusion issues with global transfer functions and
			create more useful, detailed renderings.
			See the [report](https://github.com/Twinklebear/topo-vol/blob/master/report.pdf) for more details."
	},
	{
		"title": "tray",
		"layout": 2,
		"gh_user": "Twinklebear",
		"gh_repo": "tray",
		"thumb": "https://i.imgur.com/rF58b9c.png",
		"img": [
			"https://i.imgur.com/MALPsD8.png",
			"https://i.imgur.com/Ns3ZYsE.png",
			"https://i.imgur.com/ApHMuex.png"
		],
		"description": "tray is a toy physically based ray tracer built off of the techniques discussed in
			[Physically Based Rendering](http://pbrt.org/). It currently has support for path tracing,
			bidirectional path tracing and photon mapping. tray also supports physically materials such as
			microfacet models like Torrance-Sparrow and measured data from the
			[MERL BRDF Database](http://www.merl.com/brdf/).\n\n
			This is the project tray_rust is based on and as such may not see much more development since most
			of my ray tracing work is now going on there."
	},
	{
		"title": "WebGL Volume Raycaster",
		"layout": 2,
		"gh_user": "Twinklebear",
		"gh_repo": "webgl-volume-raycaster",
        "webtoy": 1,
        "toy_url": "/webgl-volume-raycaster/",
		"img": [
			"https://i.imgur.com/yLa27hG.png",
			"https://i.imgur.com/FOUKtV1.png",
			"https://i.imgur.com/Vv98vJZ.png"
		],
		"thumb": "https://i.imgur.com/g5a0I5Y.jpg",
		"description": "A scientific visualization style volume raycaster written
			using WebGL2 and Javascript. The renderer uses an arcball camera which supports
			mouse or touch input, and dynamically adjusts the sampling rate
			to maintain a smooth framerate, even on mobile devices. The volumes
			are downloaded via XMLHttpRequest from Dropbox when selected. I've also written
			a [post](https://www.willusher.io/webgl/2019/01/13/volume-rendering-with-webgl)
			about volume rendering for scientific visualization, and how I've implemented
			in WebGL to write this renderer.
			[Try it out online!](https://www.willusher.io/webgl-volume-raycaster/)"
	},
	{
		"title": "μPacket",
		"layout": 1,
		"gh_user": "Twinklebear",
		"gh_repo": "micro-packet",
		"thumb": "https://i.imgur.com/Z4qlp42.png",
		"img": "http://i.imgur.com/WcM6Rcl.png",
		"description": "μPacket is an extremely simple packet based ray tracer that uses the AVX/AVX2 instruction
			set to trace eight rays at once through the scene. Currently it only supports spheres and planes
			with Lambertian BRDFs illuminated by a single point light. Illumination is computed with Whitted ray
			tracing, although recursion only goes as far as computing shadows since there are no reflective
			or transmissive materials at the moment.\n\n
			Ray packets were first introduced by [Wald et al., 2001](http://www.sci.utah.edu/~wald/Publications/2001/CRT/CRT.pdf)
			and are now widely used in high performance ray tracers like [Embree](http://embree.github.io/) due to
			the performance gain achieved with good packet (and now stream) tracing techniques.\n\n
			Current plans for this project are to switch to trace ray streams and add support for a path tracing
			integrator for higher quality images."
	},
	{
		"title": "DXR Ambient Occlusion Baking",
		"layout": 1,
		"gh_user": "Twinklebear",
		"gh_repo": "dxr-ao-bake",
		"img": "https://i.imgur.com/O3BSRJ9.png",
		"description": "
        A demo of ambient occlusion map baking using DXR inline ray tracing.
        Uses [xatlas](https://github.com/jpcy/xatlas) to unwrap the mesh.
        The mesh is rasterized into the atlas image by using the UV
        coordinates as the output vertex positions, and the AO factor
        computed by using inline ray tracing in the fragment shader. The image
        shows the computed AO map on Sponza."
	},
	{
		"title": "WebGL EWA Surface Splatter",
		"layout": 2,
		"gh_user": "Twinklebear",
		"gh_repo": "webgl-ewa-splatter",
        "webtoy": 1,
        "toy_url": "/webgl-ewa-splatter/",
		"img": [
			"https://i.imgur.com/yqCfPZz.png",
			"https://i.imgur.com/c6Cj6xa.png",
			"https://i.imgur.com/UBjFKRa.png"
		],
		"description": "
		An elliptical weighted average (EWA) surface splatter renderer, implemented in WebGL,
		which also supports painting on the surfaces.
		[Try it out online!](https://www.willusher.io/webgl-ewa-splatter/)
		This implements the papers: *Object Space EWA Surface Splatting: A Hardware
		Accelerated Approach to High Quality Point Rendering* by Ren, Pfister and Zwicker,
		and *High-Quality Point-Based Rendering on Modern GPUs* by Botsch and Kobbelt, with a few shortcuts.
		It also uses the deferred shading for splatting approach described
		in *High-quality surface splatting on today's GPUs*
		by Botsch, Hornung, Zwicker and Kobbelt.

		The renderer uses an arcball camera which supports mouse or touch input,
		and downloads datasets via XMLHttpRequest from Dropbox when selected.

		Built on top of [webgl-util](https://github.com/Twinklebear/webgl-util) for some WebGL utilities,
		[glMatrix](http://glmatrix.net/) for matrix/vector operations, and
		[FileSaver.js](https://github.com/eligrey/FileSaver.js/) for saving models."
	},
	{
		"title": "WebGL Marching Cubes",
		"layout": 2,
		"gh_user": "Twinklebear",
		"gh_repo": "webgl-marching-cubes",
        "webtoy": 1,
        "toy_url": "/webgl-marching-cubes/",
		"img": [
			"https://i.imgur.com/JW9UZYP.png",
			"https://i.imgur.com/aRJzV7x.png",
			"https://i.imgur.com/kW5sr9a.png"
		],
		"description": "A WebGL + WebASM implementation of the classic [Marching Cubes](https://en.wikipedia.org/wiki/Marching_cubes)
        algorithm for extracting [isosurfaces](https://en.wikipedia.org/wiki/Isosurface) from 3D volume data.
        An isosurface is a surface which represents points in the 3D data which all have the same value (e.g., pressure, temperature).
        The isosurface extraction code
        is implemented in Rust and compiled to WebAssembly to accelerate extraction of the surface. Depending on your browser, the
        WebASM version is 10-50x faster than the pure Javascript one!
        The surface is rendered as a triangle mesh and combined with the
        volume during the volume raycasting step, in a manner roughly similar to shadow mapping.
        [Try it out online!](https://www.willusher.io/webgl-marching-cubes/)"
	},
	{
		"title": "WebGPU Experiments",
		"layout": 2,
		"gh_user": "Twinklebear",
		"gh_repo": "webgpu-experiments",
        "webtoy": 1,
        "toy_url": "/webgpu-experiments/",
		"img": [
			"https://i.imgur.com/GQBJC92.png",
			"https://i.imgur.com/j21k9Z9.png",
			"https://i.imgur.com/3XMumHL.png"
		],
		"description": "
        A series of examples written while learning about [WebGPU](https://gpuweb.github.io/gpuweb/):
        a glTF viewer, a web-based LiDAR viewer, and a data-parallel Marching Cubes implementation using compute shaders.
        The glTF viewer uses a custom glb importer to load data efficiently into WebGPU and supports
        the basic glTF features. The LiDAR viewer uses [LAStools.js](https://github.com/Twinklebear/LAStools.js),
        a version of [libLAS](https://github.com/LAStools/LAStools)
        compiled to Web Assembly, to load las and laz files directly in the browser.
        The Marching Cubes example is a data-parallel implementation of marching cubes
        written using compute shaders to leverage GPU compute for interactive isosurface
        extraction. If you have a browser with WebGPU enabled, you can try them out:
        [glTF Viewer](https://www.willusher.io/webgpu-experiments/glb_viewer.html),
        [LiDAR Viewer](https://www.willusher.io/webgpu-experiments/lidar_viewer.html),
        [Marching Cubes](https://www.willusher.io/webgpu-experiments/marching_cubes.html)."
	},
	{
		"title": "WebGL Neuron Visualizer",
		"layout": 2,
		"gh_user": "Twinklebear",
		"gh_repo": "webgl-neuron",
        "webtoy": 1,
        "toy_url": "/webgl-neuron/",
		"img": [
			"https://i.imgur.com/9vVRCLE.png",
			"https://i.imgur.com/lwlbLCw.png",
			"https://i.imgur.com/bXMjno0.png"
		],
		"description": "A neuron visualization system using WebGL2. The
		renderer uses my WebGL2 volume renderer to display the RAW volume or imported TIFF
		stack and renders the imported neuron traces as lines within the volume.
		It can import single-channel 8bit and 16bit TIFF image stacks, and can
		import neuron traces in the SWC file format. TIFF files are loaded using a build
        of [libtiff compiled to WebAssembly](https://github.com/Twinklebear/tiff.js).
		The neurons are composited within the volume by rendering out the depth buffer
		and using this to terminate rays early in the volume when they hit the geometry,
		roughly similar to shadow mapping.
		A default demo dataset is included which renders a stitched version of the
		DIADEM NC Layer 1 Axons from the [DIADEM Challenge](http://diademchallenge.org/).
		[Try it out online!](https://www.willusher.io/webgl-neuron/)"
	},
	{
		"title": "Charm++ Experiments",
		"layout": 1,
		"gh_user": "Twinklebear",
		"gh_repo": "Charm-experiments",
		"img": "https://i.imgur.com/V47bIPs.png",
		"description": "A set of different test applications to learn and try
			out [Charm++](http://charmplusplus.org/) for distributed rendering applications. The image
			shown is from the [pathtracing](https://github.com/Twinklebear/Charm-experiments/tree/master/pathtracer) test application, which renders distributed data with global illumination
			by routing rays around the network. There's also a basic image-parallel
			and data-parallel scientific visualization [volume renderer](https://github.com/Twinklebear/Charm-experiments/tree/master/scivis)."
	},
	{
		"title": "bspline",
		"layout": 1,
		"gh_user": "Twinklebear",
		"gh_repo": "bspline",
		"img": "http://i.imgur.com/JvVlYKf.png",
		"description": "A Rust library for computing B-spline interpolating curves on generic control points.
			bspline can be used to evaluate B-splines of varying orders on any type that can be linearly
			interpolated, ranging from floats, positions, RGB colors to transformation matrices and so on.

			The bspline logo (above) was generated using this library with a cubic B-spline in 2D for the
			positioning of the curve and a quadratic B-spline in RGB space to color it."
	},
	{
		"title": "Spline Viewer",
		"layout": 4,
		"gh_user": "Twinklebear",
		"gh_repo": "spline-viewer",
		"img": "http://i.imgur.com/EzAQZyM.png",
		"description": "A viewer for B-spline curves and surfaces, initially written for a course on
			computer aided geometric design. You can edit and create 2D B-splines
			and tweak some properties of loaded 3D curves and surfaces. Useful for learning
			about and playing with B-splines."
	},
	{
		"title": "tobj",
		"layout": 1,
		"gh_user": "Twinklebear",
		"gh_repo": "tobj",
		"img": "http://i.imgur.com/wImyNG4.png",
		"description": "tobj is a tiny OBJ loader in Rust that draws inspiration for its API and design from
			Syoyo's excellent library, [tinyobjloader](https://github.com/syoyo/tinyobjloader).
			The crate aims to be a simple, fast and
			lightweight option for loading OBJ and MTL files for easy integration with realtime and offline
			renderers, or really any other project where you need to load OBJ files!\n\n
			The image shown is from a demo viewer written to test tobj named
			[tobj_viewer](https://github.com/Twinklebear/tobj_viewer) displaying the
			[Rungholt](http://graphics.cs.williams.edu/data/meshes.xml) model. The model can be found on
			Morgan McGuire's meshes page and was originally built by kescha."
	},
	{
		"title": "ssao",
		"layout": 7,
		"gh_user": "Twinklebear",
		"gh_repo": "ssao",
		"img": [
			"http://i.imgur.com/byM8iNh.png",
			"http://i.imgur.com/v7wWg9O.png"
		],
		"description": "This is sort of an implementation of
			[Scalable Ambient Obscurance by McGuire et al.](http://graphics.cs.williams.edu/papers/SAOHPG12/)
			however I make a few simplifying shortcuts in my implementation and don't achieve as good performance
			or quality. I was also unable to get their new recommended estimator to behave so this implementation
			still uses the Alchemy AO estimator initially recommended in the paper. There's a somewhat longer write
			up available on [my classpage](http://www.willusher.io/courses/cs6610/) since this was initially
			implemented as a class project.\n\n
			The image shows just the ambient occlusion value for a view of the Crytek Sponza scene."
	},
	{
		"title": "lfwatch",
		"layout": 5,
		"gh_user": "Twinklebear",
		"gh_repo": "lfwatch",
		"description": "lfwatch is a lightweight file watcher for Windows, Linux and OS X. It monitors some
			desired directories for file changes and calls the callback set for the directory with information
			about the file change event. This was written to do hot reloading of GLSL shaders but could be
			useful for some other applications."
	}
]

